{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNveEhM2eZsZ"
   },
   "source": [
    "# JamUNet model trained with the spatial dataset - training and validation\n",
    "\n",
    "This notebook was used for training and validating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathi\\Desktop\\TU Delft\\TU Delft year 5\\Data_science\\Morphology_project\\jamunet-morpho-braided\n"
     ]
    }
   ],
   "source": [
    "%cd c:\\Users\\mathi\\Desktop\\TU Delft\\TU Delft year 5\\Data_science\\Morphology_project\\jamunet-morpho-braided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "executionInfo": {
     "elapsed": 5735,
     "status": "error",
     "timestamp": 1765273111241,
     "user": {
      "displayName": "Mathias Ruhe",
      "userId": "08308838018676251424"
     },
     "user_tz": -60
    },
    "id": "dJHSJLrkeZsh",
    "outputId": "1a94d5f2-c92b-4cd9-a7fe-d1e6d0141079"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Current working directory: c:\\Users\\mathi\\Desktop\\TU Delft\\TU Delft year 5\\Data_science\\Morphology_project\\jamunet-morpho-braided\n"
     ]
    }
   ],
   "source": [
    "# import modules\n",
    "import os\n",
    "import torch\n",
    "import joblib\n",
    "import copy\n",
    "import warnings\n",
    "from torch.utils.data import DataLoader, TensorDataset, ConcatDataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from model.train_eval import *\n",
    "from preprocessing.dataset_generation import create_full_dataset\n",
    "from postprocessing.save_results import *\n",
    "from postprocessing.plot_results import *\n",
    "\n",
    "# enable interactive widgets in Jupyter Notebook\n",
    "%matplotlib inline\n",
    "%matplotlib widget\n",
    "\n",
    "# reload modules to avoid restarting the notebook every time these are updated\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "UOIKPAR6eZsk",
    "outputId": "255faef6-4d9e-4fb8-c8c9-9e2509ec35c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Device Count:  1\n",
      "CUDA Device Name:  Quadro P2000\n",
      "Using device: cuda:0\n",
      "\n",
      "Logical CPU cores: 12\n"
     ]
    }
   ],
   "source": [
    "# set the device where operations are performed\n",
    "# if only one GPU is present you might need to remove the index \"0\"\n",
    "# torch.device('cuda:0') --> torch.device('cuda') / torch.cuda.get_device_name(0) --> torch.cuda.get_device_name()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(\"CUDA Device Count: \", torch.cuda.device_count())\n",
    "    print(\"CUDA Device Name: \", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "num_cpus = os.cpu_count()  # total logical cores\n",
    "print(\"\\nLogical CPU cores:\", num_cpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified code\n",
    "\n",
    "This code should be integrated into the dataset_generation.py file but failed multiple times without finding out what the exact error was. Therefore The function have been put in the ipynb file to safely loop over them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Datasets for MONTH 4...\n",
      "\n",
      "River: Jamuna\n",
      "    Reading from: data\\satellite\\Jamuna_images\\dataset_month4\n",
      "River: Ganges\n",
      "    Reading from: data\\satellite\\Ganges_images\\preprocessed\\month_4\n",
      "River: Indus\n",
      "    Reading from: data\\satellite\\Indus_images\\preprocessed\\month_4\n",
      "River: Ghangara\n",
      "    Reading from: data\\satellite\\Ghangara_images\\preprocessed\\month_4\n",
      "\n",
      "========================================\n",
      "FINAL SUMMARY (Month 4)\n",
      "========================================\n",
      "Training Set:   703 samples\n",
      "Validation Set: 18 samples\n",
      "Testing Set:    18 samples\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ==========================================\n",
    "# 1. HELPER FUNCTIONS\n",
    "# ==========================================\n",
    "\n",
    "def load_image_array(path, scaled_classes=True):\n",
    "    '''Load a single image using Gdal and convert to float32 array.'''\n",
    "    ds = gdal.Open(path)\n",
    "    if ds is None: return None\n",
    "    img_array = ds.ReadAsArray().astype(np.float32)\n",
    "\n",
    "    if scaled_classes:\n",
    "        img_array = img_array.astype(int)\n",
    "        img_array[img_array==0] = -1\n",
    "        img_array[img_array==1] = 0\n",
    "        img_array[img_array==2] = 1\n",
    "    \n",
    "    return img_array\n",
    "\n",
    "def load_avg(train_val_test, reach, year, dir_averages):\n",
    "    '''Robustly loads the average image for a given year.'''\n",
    "    # 1. Try standard path\n",
    "    folder_name = f'average_{train_val_test}_r{reach}'\n",
    "    filename = f'average_{year}_{train_val_test}_r{reach}.csv'\n",
    "    full_path = os.path.join(dir_averages, folder_name, filename)\n",
    "    \n",
    "    if os.path.exists(full_path):\n",
    "        return pd.read_csv(full_path, header=None).to_numpy()\n",
    "    \n",
    "    # 2. Fallback: Try finding it in generic folders\n",
    "    potential_folders = [f'average_training_r{reach}', f'average_r{reach}']\n",
    "    for folder in potential_folders:\n",
    "        path_to_check = os.path.join(dir_averages, folder, filename)\n",
    "        if os.path.exists(path_to_check):\n",
    "            return pd.read_csv(path_to_check, header=None).to_numpy()\n",
    "            \n",
    "    return None\n",
    "\n",
    "def create_list_images(train_val_test, reach, dir_folders, collection):\n",
    "    '''Finds the correct reach folder and returns list of .tif images.'''\n",
    "    list_dir_images = []\n",
    "    \n",
    "    if not os.path.exists(dir_folders):\n",
    "        print(f\"   ❌ Error: Directory not found: {dir_folders}\")\n",
    "        return []\n",
    "\n",
    "    target_folder_path = None\n",
    "    \n",
    "    # Search for folder ending in \"_rX\"\n",
    "    for folder_name in os.listdir(dir_folders):\n",
    "        if folder_name.endswith(f'_r{reach}'):\n",
    "            # If a specific filter (like 'training') is requested, prioritize it\n",
    "            if train_val_test in folder_name or train_val_test == 'training':\n",
    "                target_folder_path = os.path.join(dir_folders, folder_name)\n",
    "                break\n",
    "    \n",
    "    # Fallback: Just match reach ID if specific tag not found\n",
    "    if target_folder_path is None:\n",
    "        for folder_name in os.listdir(dir_folders):\n",
    "            if folder_name.endswith(f'_r{reach}'):\n",
    "                target_folder_path = os.path.join(dir_folders, folder_name)\n",
    "                break\n",
    "\n",
    "    if target_folder_path is None:\n",
    "        return []\n",
    "\n",
    "    # Collect images\n",
    "    sorted_files = sorted(os.listdir(target_folder_path))\n",
    "    for image in sorted_files:\n",
    "        if image.endswith('.tif'):\n",
    "            list_dir_images.append(os.path.join(target_folder_path, image))\n",
    "            \n",
    "    return list_dir_images\n",
    "\n",
    "# ==========================================\n",
    "# 2. DATASET LOGIC\n",
    "# ==========================================\n",
    "\n",
    "def create_datasets(train_val_test, reach, year_target=5, nodata_value=-1, dir_folders=r'data\\satellite\\dataset', \n",
    "                    collection=r'JRC_GSW1_4_MonthlyHistory', scaled_classes=True):\n",
    "    \n",
    "    # 1. Get Images\n",
    "    list_dir_images = create_list_images(train_val_test, reach, dir_folders, collection)\n",
    "    if not list_dir_images: return [], [] \n",
    "\n",
    "    # 2. Load Images\n",
    "    images_array = []\n",
    "    loaded_years = []\n",
    "    \n",
    "    for idx, path in enumerate(list_dir_images):\n",
    "        img = load_image_array(path, scaled_classes=scaled_classes)\n",
    "        if img is not None:\n",
    "            images_array.append(img)\n",
    "            filename = os.path.basename(path)\n",
    "            try:\n",
    "                # Extract year (4 digits)\n",
    "                parts = filename.replace('-', '_').split('_')\n",
    "                year = next(p for p in parts if p.isdigit() and len(p) == 4)\n",
    "                loaded_years.append(int(year))\n",
    "            except:\n",
    "                loaded_years.append(1988 + idx)\n",
    "\n",
    "    # 3. Load Averages (DYNAMIC PATH LOGIC)\n",
    "    # Go up: month_X -> preprocessed -> {River}_images -> averages\n",
    "    parent_dir = os.path.dirname(dir_folders)\n",
    "    river_base_dir = os.path.dirname(parent_dir)\n",
    "    dir_averages_dynamic = os.path.join(river_base_dir, 'averages')\n",
    "\n",
    "    avg_imgs = []\n",
    "    for year in loaded_years:\n",
    "        avg = load_avg(train_val_test, reach, year, dir_averages=dir_averages_dynamic)\n",
    "        if avg is None:\n",
    "            if len(images_array) > 0: avg = np.zeros_like(images_array[0])\n",
    "            else: return [], []\n",
    "        avg_imgs.append(avg)\n",
    "\n",
    "    # 4. Replace No-Data\n",
    "    good_images_array = [np.where(image == nodata_value, avg_imgs[i], image) \n",
    "                         for i, image in enumerate(images_array)]\n",
    "        \n",
    "    # 5. Create Sequences (n-to-1)\n",
    "    input_dataset = []\n",
    "    target_dataset = []\n",
    "    \n",
    "    if len(good_images_array) < year_target: return [], []\n",
    "\n",
    "    for i in range(len(good_images_array) - year_target + 1):\n",
    "        input_dataset.append(good_images_array[i : i + year_target - 1])\n",
    "        target_dataset.append([good_images_array[i + year_target - 1]])\n",
    "\n",
    "    return input_dataset, target_dataset\n",
    "\n",
    "def combine_datasets(train_val_test, reach, year_target=5, nonwater_threshold=480000, nodata_value=-1, nonwater_value=0,   \n",
    "                     dir_folders=r'data\\satellite\\dataset', collection=r'JRC_GSW1_4_MonthlyHistory', scaled_classes=True):\n",
    "    \n",
    "    # Create raw dataset\n",
    "    input_dataset, target_dataset = create_datasets(\n",
    "        train_val_test, reach, year_target, nodata_value, \n",
    "        dir_folders, collection, scaled_classes\n",
    "    )\n",
    "\n",
    "    filtered_inputs = []\n",
    "    filtered_targets = []\n",
    "\n",
    "    # Filter logic\n",
    "    for input_images, target_image_seq in zip(input_dataset, target_dataset):\n",
    "        is_input_good = True\n",
    "        for img in input_images:\n",
    "            if np.sum(img == nonwater_value) >= nonwater_threshold:\n",
    "                is_input_good = False\n",
    "                break\n",
    "        \n",
    "        if is_input_good:\n",
    "            target_img = target_image_seq[0]\n",
    "            if np.sum(target_img == nonwater_value) < nonwater_threshold:\n",
    "                filtered_inputs.append(input_images)\n",
    "                filtered_targets.append(target_img)\n",
    "\n",
    "    return filtered_inputs, filtered_targets\n",
    "\n",
    "def create_full_dataset(train_val_test, year_target=5, nonwater_threshold=480000, nodata_value=-1, nonwater_value=0, \n",
    "                        dir_folders=None, name_filter=None, collection=r'JRC_GSW1_4_MonthlyHistory', \n",
    "                        scaled_classes=True, device='cpu', dtype=torch.float32):\n",
    "    \n",
    "    all_inputs = []\n",
    "    all_targets = []\n",
    "    \n",
    "    if not os.path.exists(dir_folders):\n",
    "        print(f\"   ⚠️ Path not found: {dir_folders}\")\n",
    "        return TensorDataset(torch.empty(0), torch.empty(0))\n",
    "\n",
    "    potential_folders = [f for f in os.listdir(dir_folders) if os.path.isdir(os.path.join(dir_folders, f))]\n",
    "    \n",
    "    for folder_name in potential_folders:\n",
    "        # FILTER: If name_filter is set (e.g. 'validation'), folder MUST contain it\n",
    "        if name_filter and name_filter not in folder_name:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            reach_id = int(folder_name.split('_r')[-1])\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        use_label = name_filter if name_filter else train_val_test\n",
    "        \n",
    "        inputs, targets = combine_datasets(\n",
    "            train_val_test=use_label, \n",
    "            reach=reach_id, \n",
    "            year_target=year_target, \n",
    "            nonwater_threshold=nonwater_threshold,\n",
    "            nodata_value=nodata_value, \n",
    "            nonwater_value=nonwater_value, \n",
    "            dir_folders=dir_folders, \n",
    "            collection=collection, \n",
    "            scaled_classes=scaled_classes\n",
    "        )\n",
    "        \n",
    "        if len(inputs) > 0:\n",
    "            all_inputs.extend(inputs)\n",
    "            all_targets.extend(targets)\n",
    "\n",
    "    if not all_inputs:\n",
    "        return TensorDataset(torch.empty(0), torch.empty(0))\n",
    "\n",
    "    # Convert to Tensor (Using CPU to avoid OOM)\n",
    "    input_tensor = torch.tensor(np.array(all_inputs), dtype=dtype, device=device)\n",
    "    target_tensor = torch.tensor(np.array(all_targets), dtype=dtype, device=device)\n",
    "    \n",
    "    return TensorDataset(input_tensor, target_tensor)\n",
    "\n",
    "# ==========================================\n",
    "# 3. MASTER EXECUTION SCRIPT\n",
    "# ==========================================\n",
    "\n",
    "# CONFIG\n",
    "target_month = 4\n",
    "device = 'cpu' # Keep data on CPU\n",
    "dtype = torch.float32\n",
    "base_dir = os.path.join('data', 'satellite')\n",
    "\n",
    "train_lists, val_lists, test_lists = [], [], []\n",
    "\n",
    "print(f\"Building Datasets for MONTH {target_month}...\\n\")\n",
    "\n",
    "rivers = ['Jamuna', 'Ganges', 'Indus', 'Ghangara']\n",
    "\n",
    "for river in rivers:\n",
    "    print(f\"River: {river}\")\n",
    "    \n",
    "    # Intelligent Path Finder\n",
    "    possible_paths = [\n",
    "        os.path.join(base_dir, f'{river}_images',  f'dataset_month{target_month}'),\n",
    "        os.path.join(base_dir, f'{river}_images', f'dataset_month_{target_month}'),\n",
    "        os.path.join(base_dir, f'{river}_images', 'preprocessed', f'month_{target_month}'),\n",
    "        os.path.join(base_dir, f'{river}_images', 'preprocessed', f'month{target_month}')\n",
    "    ]\n",
    "    \n",
    "    source_path = None\n",
    "    for p in possible_paths:\n",
    "        if os.path.exists(p):\n",
    "            source_path = p\n",
    "            break\n",
    "            \n",
    "    if source_path is None:\n",
    "        print(f\"    SKIPPING: Could not find month folder.\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"    Reading from: {source_path}\")\n",
    "\n",
    "    # Generate Datasets\n",
    "    if river == 'Jamuna':\n",
    "        # Split Jamuna\n",
    "        ds_tr = create_full_dataset('training', dir_folders=source_path, name_filter='training', device=device, dtype=dtype)\n",
    "        if len(ds_tr) > 0: train_lists.append(ds_tr)\n",
    "            \n",
    "        ds_val = create_full_dataset('validation', dir_folders=source_path, name_filter='validation', device=device, dtype=dtype)\n",
    "        if len(ds_val) > 0: val_lists.append(ds_val)\n",
    "\n",
    "        ds_test = create_full_dataset('testing', dir_folders=source_path, name_filter='testing', device=device, dtype=dtype)\n",
    "        if len(ds_test) > 0: test_lists.append(ds_test)\n",
    "        \n",
    "    else:\n",
    "        # Others -> Training\n",
    "        ds_tr = create_full_dataset('training', dir_folders=source_path, name_filter=None, device=device, dtype=dtype)\n",
    "        if len(ds_tr) > 0: train_lists.append(ds_tr)\n",
    "\n",
    "# Final Merge\n",
    "final_train_set = ConcatDataset(train_lists) if train_lists else None\n",
    "final_val_set = ConcatDataset(val_lists) if val_lists else None\n",
    "final_test_set = ConcatDataset(test_lists) if test_lists else None\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"FINAL SUMMARY (Month {target_month})\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Training Set:   {len(final_train_set) if final_train_set else 0} samples\")\n",
    "print(f\"Validation Set: {len(final_val_set) if final_val_set else 0} samples\")\n",
    "print(f\"Testing Set:    {len(final_test_set) if final_test_set else 0} samples\")\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cR0CKQUseZsx"
   },
   "source": [
    "**<span style=\"color:red\">Attention!</span>**\n",
    "\\\n",
    "Uncomment the next cells if larger training, validation, and testing datasets are needed. These cells load all months datasets (January, February, March, and April) and then merge them into one dataset.\n",
    "\\\n",
    "Keep in mind that due to memory constraints, it is likely that not all four datasets can be loaded.\n",
    "\\\n",
    "Make sure to load the training, validation, and testing datasets in different cells to reduce memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "v3axPWqKeZsy"
   },
   "outputs": [],
   "source": [
    "# dataset_jan = r'data\\satellite\\dataset_month1'\n",
    "# dataset_feb = r'data\\satellite\\dataset_month2'\n",
    "# dataset_mar = r'data\\satellite\\dataset_month3'\n",
    "# dataset_apr = r'data\\satellite\\dataset_month4'\n",
    "\n",
    "# dtype=torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "bb6PlXx2eZsz"
   },
   "outputs": [],
   "source": [
    "# inputs_train_jan, targets_train_jan = create_full_dataset(train, dir_folders=dataset_jan, device=device, dtype=dtype).tensors\n",
    "# inputs_train_feb, targets_train_feb = create_full_dataset(train, dir_folders=dataset_feb, device=device, dtype=dtype).tensors\n",
    "# inputs_train_mar, targets_train_mar = create_full_dataset(train, dir_folders=dataset_mar, device=device, dtype=dtype).tensors\n",
    "# inputs_train_apr, targets_train_apr = create_full_dataset(train, dir_folders=dataset_apr, device=device, dtype=dtype).tensors\n",
    "\n",
    "# inputs_train = torch.cat((inputs_train_jan, inputs_train_feb, inputs_train_mar, inputs_train_apr))\n",
    "# targets_train = torch.cat((targets_train_jan, targets_train_feb, targets_train_mar, targets_train_apr))\n",
    "# train_set = TensorDataset(inputs_train, targets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "VSq1Ko5ceZs0"
   },
   "outputs": [],
   "source": [
    "# inputs_val_jan, targets_val_jan = create_full_dataset(val, dir_folders=dataset_jan, device=device, dtype=dtype).tensors\n",
    "# inputs_val_feb, targets_val_feb = create_full_dataset(val, dir_folders=dataset_feb, device=device, dtype=dtype).tensors\n",
    "# inputs_val_mar, targets_val_mar = create_full_dataset(val, dir_folders=dataset_mar, device=device, dtype=dtype).tensors\n",
    "# inputs_val_apr, targets_val_apr = create_full_dataset(val, dir_folders=dataset_apr, device=device, dtype=dtype).tensors\n",
    "\n",
    "# inputs_val = torch.cat((inputs_val_jan, inputs_val_feb, inputs_val_mar, inputs_val_apr))\n",
    "# targets_val = torch.cat((targets_val_jan, targets_val_feb, targets_val_mar, targets_val_apr))\n",
    "# val_set = TensorDataset(inputs_val, targets_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "OagE3pHPeZs1"
   },
   "outputs": [],
   "source": [
    "# inputs_test_jan, targets_test_jan = create_full_dataset(test, dir_folders=dataset_jan, device=device, dtype=dtype).tensors\n",
    "# inputs_test_feb, targets_test_feb = create_full_dataset(test, dir_folders=dataset_feb, device=device, dtype=dtype).tensors\n",
    "# inputs_test_mar, targets_test_mar = create_full_dataset(test, dir_folders=dataset_mar, device=device, dtype=dtype).tensors\n",
    "# inputs_test_apr, targets_test_apr = create_full_dataset(test, dir_folders=dataset_apr, device=device, dtype=dtype).tensors\n",
    "\n",
    "# inputs_test = torch.cat((inputs_test_jan, inputs_test_feb, inputs_test_mar, inputs_test_apr))\n",
    "# targets_test = torch.cat((targets_test_jan, targets_test_feb, targets_test_mar, targets_test_apr))\n",
    "# test_set = TensorDataset(inputs_test, targets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsC1zKs1eZs2"
   },
   "source": [
    "**<span style=\"color:red\">Attention!</span>**\n",
    "\\\n",
    "It is not needed to scale and normalize the dataset as the pixel values are already $[0, 1]$.\n",
    "\\\n",
    "If scaling and normalization are performed anyways, then **the model inputs have to be changed** as the normalized datasets are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "RQSQywxZeZs3"
   },
   "outputs": [],
   "source": [
    "# normalize inputs and targets using the training dataset\n",
    "\n",
    "# scaler_x, scaler_y = scaler(train_set)\n",
    "\n",
    "# normalized_train_set = normalize_dataset(train_set, scaler_x, scaler_y)\n",
    "# normalized_val_set = normalize_dataset(val_set, scaler_x, scaler_y)\n",
    "# normalized_test_set = normalize_dataset(test_set, scaler_x, scaler_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "r8uGh0ENeZs3"
   },
   "outputs": [],
   "source": [
    "# save scalers to be loaded in seperate notebooks (i.e., for testing the model)\n",
    "# should not change unless seed is changed or augmentation increased (randomsplit changes)\n",
    "\n",
    "# joblib.dump(scaler_x, r'model\\scalers\\scaler_x.joblib')\n",
    "# joblib.dump(scaler_y, r'model\\scalers\\scaler_y.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "tI5dXulleZs4"
   },
   "outputs": [],
   "source": [
    "# load JamUNet architecture\n",
    "\n",
    "from model.st_unet.st_unet_3D import *\n",
    "\n",
    "n_channels = final_train_set[0][0].shape[0]\n",
    "n_classes = 1\n",
    "init_hid_dim = 8\n",
    "kernel_size = 3\n",
    "pooling = 'max'\n",
    "\n",
    "model = UNet3D(n_channels=n_channels, n_classes=n_classes, init_hid_dim=init_hid_dim,\n",
    "               kernel_size=kernel_size, pooling=pooling, bilinear=False, drop_channels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "_OydeTQGeZs4",
    "outputId": "551a098e-58b0-434f-fdd5-4a3bd0ead783"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet3D(\n",
       "  (inc): DoubleConv(\n",
       "    (net): Sequential(\n",
       "      (0): Conv3d(1, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv3d(8, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (4): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (down1): Down(\n",
       "    (net): Sequential(\n",
       "      (0): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (net): Sequential(\n",
       "          (0): Conv3d(8, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (4): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down2): Down(\n",
       "    (net): Sequential(\n",
       "      (0): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (net): Sequential(\n",
       "          (0): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (4): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down3): Down(\n",
       "    (net): Sequential(\n",
       "      (0): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (net): Sequential(\n",
       "          (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down4): Down(\n",
       "    (net): Sequential(\n",
       "      (0): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (net): Sequential(\n",
       "          (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (4): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (temporal): Sequential(\n",
       "    (0): Conv3d(128, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "    (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv3d(128, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "    (4): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (up1): Up(\n",
       "    (up): ConvTranspose3d(128, 64, kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
       "    (conv): DoubleConv(\n",
       "      (net): Sequential(\n",
       "        (0): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up2): Up(\n",
       "    (up): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
       "    (conv): DoubleConv(\n",
       "      (net): Sequential(\n",
       "        (0): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (4): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up3): Up(\n",
       "    (up): ConvTranspose3d(32, 16, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    (conv): DoubleConv(\n",
       "      (net): Sequential(\n",
       "        (0): Conv3d(32, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (4): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up4): Up(\n",
       "    (up): ConvTranspose3d(16, 8, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    (conv): DoubleConv(\n",
       "      (net): Sequential(\n",
       "        (0): Conv3d(16, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv3d(8, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (4): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (outc): OutConv(\n",
       "    (conv): Conv3d(8, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "  )\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print model architecture\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "IDDXcZQeeZs5",
    "outputId": "cbe96de2-3a28-41e8-cbd7-329002ba33c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 1.47e+06\n",
      "Model size: 5.61 MB\n"
     ]
    }
   ],
   "source": [
    "# print total number of parameters and model size\n",
    "\n",
    "num_parameters = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {num_parameters:.2e}\")\n",
    "model_size_MB = num_parameters * 4 / (1024 ** 2)  # assuming float32 precision\n",
    "print(f\"Model size: {model_size_MB:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JG6497ReZs6"
   },
   "source": [
    "**<span style=\"color:red\">Attention!</span>**\n",
    "\\\n",
    "Since it is not needed to scale and normalize the dataset (see above), the input for the Data Loader are not the normalized datasets.\n",
    "\\\n",
    "If normalization is performed, the normalized datasets become the inputs to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "f6TmF-HZeZs7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loader Ready!\n",
      "Val Loader Ready!\n",
      "Test Loader Ready!\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "learning_rate = 0.05\n",
    "batch_size = 16\n",
    "num_epochs = 1\n",
    "water_threshold = 0.5\n",
    "physics = False    # no physics-induced loss terms in the training loss if False\n",
    "alpha_er = 1e-4    # needed only if physics=True\n",
    "alpha_dep = 1e-4   # needed only if physics=True\n",
    "\n",
    "# optimizer to train the model with backpropagation\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "# scheduler for decreasing the learning rate\n",
    "# every tot epochs (step_size) with given factor (gamma)\n",
    "step_size = 15     # set to None to remove the scheduler\n",
    "gamma = 0.75       # set to None to remove the scheduler\n",
    "if (step_size and gamma) is not None:\n",
    "    scheduler = StepLR(optimizer, step_size = step_size, gamma = gamma)\n",
    "\n",
    "# dataloaders to input data to the model in batches -- see note above if normalization is performed\n",
    "# Create DataLoaders\n",
    "batch_size = 16\n",
    "if final_train_set and len(final_train_set) > 0:\n",
    "    train_loader = DataLoader(final_train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    print(\"Train Loader Ready!\")\n",
    "if final_val_set and len(final_val_set) > 0:\n",
    "    val_loader = DataLoader(final_val_set, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    print(\"Val Loader Ready!\")\n",
    "if final_test_set and len(final_test_set) > 0:\n",
    "    test_loader = DataLoader(final_test_set, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    print(\"Test Loader Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "DWk-h6XoeZs7",
    "outputId": "86477ffe-21f7-47a9-b3b0-8d63966b7b6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ GPU test successful: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Test GPU before training\n",
    "try:\n",
    "    test_tensor = torch.randn(2, 2).to(device)\n",
    "    print(f\"✓ GPU test successful: {device}\")\n",
    "    del test_tensor\n",
    "    torch.cuda.empty_cache()\n",
    "except Exception as e:\n",
    "    print(f\"✗ GPU test failed: {e}\")\n",
    "    print(\"Switching to CPU...\")\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation samples: 18\n",
      "Batches in val_loader: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Validation samples: {len(final_val_set)}\")\n",
    "print(f\"Batches in val_loader: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "oa-V-WKteZs-",
    "outputId": "f9d10261-8595-468e-bb4e-84f56ea35bad"
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.79 GiB is allocated by PyTorch, and 98.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# update the learning rate\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# model training\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_unet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwater_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwater_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_f\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphysics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mphysics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_er\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha_er\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m                           \u001b[49m\u001b[43malpha_dep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha_dep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_er_dep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_er_dep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# model validation\u001b[39;00m\n\u001b[0;32m     24\u001b[0m val_loss, val_accuracy, val_precision, val_recall, val_f1_score, val_csi_score \u001b[38;5;241m=\u001b[39m validation_unet(model, val_loader,\n\u001b[0;32m     25\u001b[0m                                                                                                  device\u001b[38;5;241m=\u001b[39mdevice, loss_f\u001b[38;5;241m=\u001b[39mloss_f,\n\u001b[0;32m     26\u001b[0m                                                                                                  water_threshold\u001b[38;5;241m=\u001b[39mwater_threshold)\n",
      "File \u001b[1;32mc:\\Users\\mathi\\Desktop\\TU Delft\\TU Delft year 5\\Data_science\\Morphology_project\\jamunet-morpho-braided\\model\\train_eval.py:94\u001b[0m, in \u001b[0;36mtraining_unet\u001b[1;34m(model, loader, optimizer, nonwater, water, pixel_size, water_threshold, device, loss_f, physics, alpha_er, alpha_dep, loss_er_dep)\u001b[0m\n\u001b[0;32m     91\u001b[0m target \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# 1. Forward pass\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mget_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# 2. Compute binary classification loss\u001b[39;00m\n\u001b[0;32m     97\u001b[0m binary_loss \u001b[38;5;241m=\u001b[39m choose_loss(predictions, target, loss_f)\n",
      "File \u001b[1;32mc:\\Users\\mathi\\Desktop\\TU Delft\\TU Delft year 5\\Data_science\\Morphology_project\\jamunet-morpho-braided\\model\\train_eval.py:41\u001b[0m, in \u001b[0;36mget_predictions\u001b[1;34m(model, input_dataset, device)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_predictions\u001b[39m(model, input_dataset, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m    Compute the predictions given the deep-learning model class and input dataset\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m           predictions = list, model predictions\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m    \n\u001b[1;32m---> 41\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mathi\\miniconda3\\envs\\braided\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mathi\\miniconda3\\envs\\braided\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mathi\\Desktop\\TU Delft\\TU Delft year 5\\Data_science\\Morphology_project\\jamunet-morpho-braided\\model\\st_unet\\st_unet_3D.py:110\u001b[0m, in \u001b[0;36mUNet3D.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    107\u001b[0m x5 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal(x5)\n\u001b[0;32m    109\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup1(x5, x4)\n\u001b[1;32m--> 110\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup3(x, x2)\n\u001b[0;32m    112\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup4(x, x1)\n",
      "File \u001b[1;32mc:\\Users\\mathi\\miniconda3\\envs\\braided\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mathi\\miniconda3\\envs\\braided\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mathi\\Desktop\\TU Delft\\TU Delft year 5\\Data_science\\Morphology_project\\jamunet-morpho-braided\\model\\st_unet\\st_unet_3D.py:51\u001b[0m, in \u001b[0;36mUp.forward\u001b[1;34m(self, x1, x2)\u001b[0m\n\u001b[0;32m     45\u001b[0m dW \u001b[38;5;241m=\u001b[39m x2\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m4\u001b[39m) \u001b[38;5;241m-\u001b[39m x1\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m     47\u001b[0m x1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(x1, [dW \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, dW \u001b[38;5;241m-\u001b[39m dW \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     48\u001b[0m                 dH \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, dH \u001b[38;5;241m-\u001b[39m dH \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     49\u001b[0m                 dT \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, dT \u001b[38;5;241m-\u001b[39m dT \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mathi\\miniconda3\\envs\\braided\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mathi\\miniconda3\\envs\\braided\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mathi\\Desktop\\TU Delft\\TU Delft year 5\\Data_science\\Morphology_project\\jamunet-morpho-braided\\model\\st_unet\\st_unet_3D.py:18\u001b[0m, in \u001b[0;36mDoubleConv.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mathi\\miniconda3\\envs\\braided\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mathi\\miniconda3\\envs\\braided\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mathi\\miniconda3\\envs\\braided\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mathi\\miniconda3\\envs\\braided\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mathi\\miniconda3\\envs\\braided\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mathi\\miniconda3\\envs\\braided\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    186\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mathi\\miniconda3\\envs\\braided\\lib\\site-packages\\torch\\nn\\functional.py:2812\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m   2810\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m-> 2812\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2813\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2820\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2822\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.79 GiB is allocated by PyTorch, and 98.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# initialize training, validation losses and metrics\n",
    "train_losses, val_losses = [], []\n",
    "accuracies, precisions, recalls, f1_scores, csi_scores = [], [], [], [], []\n",
    "count = 0\n",
    "\n",
    "# set classification loss - possible options: 'BCE', 'BCE_Logits', and 'Focal'\n",
    "loss_f = 'BCE'\n",
    "# set regression loss for physics-induced terms\n",
    "# possible options: 'Huber', 'RMSE', and 'MAE'\n",
    "loss_er_dep = 'Huber'\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "\n",
    "    # update learning rate\n",
    "    if (step_size and gamma) is not None:\n",
    "        scheduler.step() # update the learning rate\n",
    "\n",
    "    # model training\n",
    "    train_loss = training_unet(model, train_loader, optimizer, water_threshold=water_threshold,\n",
    "                               device=device, loss_f=loss_f, physics=physics, alpha_er=alpha_er,\n",
    "                               alpha_dep=alpha_dep, loss_er_dep=loss_er_dep)\n",
    "\n",
    "    # model validation\n",
    "    val_loss, val_accuracy, val_precision, val_recall, val_f1_score, val_csi_score = validation_unet(model, val_loader,\n",
    "                                                                                                     device=device, loss_f=loss_f,\n",
    "                                                                                                     water_threshold=water_threshold)\n",
    "\n",
    "    if epoch == 1:\n",
    "        best_loss = val_loss\n",
    "        best_recall = val_recall\n",
    "\n",
    "    # save model with min val loss\n",
    "    if val_loss<=best_loss:\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "        count = 0\n",
    "    # save model with max recall\n",
    "    if val_recall>=best_recall:\n",
    "        best_model_recall = copy.deepcopy(model)\n",
    "        best_recall = val_recall\n",
    "        best_epoch = epoch\n",
    "        count = 0\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    accuracies.append(val_accuracy)\n",
    "    precisions.append(val_precision)\n",
    "    recalls.append(val_recall)\n",
    "    f1_scores.append(val_f1_score)\n",
    "    csi_scores.append(val_csi_score)\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    if epoch%1 == 0:\n",
    "        print(f\"Epoch: {epoch} | \" +\n",
    "              f\"Training loss: {train_loss:.2e}, Validation loss: {val_loss:.2e}, Best validation loss: {best_loss:.2e} \" +\n",
    "              f\" | Metrics: Accuracy: {val_accuracy:.3f}, Precision: {val_precision:.3f}, Recall: {val_recall:.3f},\\\n",
    " F1-score: {val_f1_score:.3f}, CSI-score: {val_csi_score:.3f}, Best recall: {best_recall:.3f}\")\n",
    "        if (step_size and gamma) is not None:\n",
    "            print(f'Current learning rate: {scheduler.get_last_lr()[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pBAWRE1zeZs_"
   },
   "outputs": [],
   "source": [
    "metrics = [accuracies, precisions, recalls, f1_scores, csi_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_FkFkSjOeZtA"
   },
   "outputs": [],
   "source": [
    "# store training and validation losses and metrics to be stored in a .csv file for later postprocessing\n",
    "# always check the dataset month key\n",
    "\n",
    "save_losses_metrics(train_losses, val_losses, metrics, 'spatial', model, 3, init_hid_dim,\n",
    "                    kernel_size, pooling, learning_rate, step_size, gamma, batch_size, num_epochs,\n",
    "                    water_threshold, physics, alpha_er, alpha_dep, dir_output=r'model\\losses_metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yaihUn6keZtA"
   },
   "source": [
    "**<span style=\"color:red\">Attention!</span>**\n",
    "\\\n",
    "Always remember to rename the <code>save_path</code> file before running the whole notebook to avoid overwrting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfeizotReZtA"
   },
   "outputs": [],
   "source": [
    "# save model with min validation loss\n",
    "# always check the dataset month key\n",
    "\n",
    "save_model_path(best_model, 'spatial', 'loss', 3, init_hid_dim, kernel_size, pooling, learning_rate,\n",
    "                step_size, gamma, batch_size, num_epochs, water_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g1qP_5oneZtB"
   },
   "outputs": [],
   "source": [
    "# save model with max recall\n",
    "# always check the dataset month key\n",
    "\n",
    "save_model_path(best_model_recall, 'spatial', 'recall', 3, init_hid_dim, kernel_size, pooling, learning_rate,\n",
    "                step_size, gamma, batch_size, num_epochs, water_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x_KmUfhweZtC"
   },
   "outputs": [],
   "source": [
    "# test the min loss model - average loss and metrics\n",
    "\n",
    "model_loss = copy.deepcopy(best_model)\n",
    "test_loss, test_accuracy, test_precision, test_recall, test_f1_score, test_csi_score = validation_unet(model_loss, test_loader, device=device, loss_f = loss_f)\n",
    "\n",
    "print(f'Average metrics for test dataset using model with best validation loss:\\n\\n\\\n",
    "{loss_f} loss:          {test_loss:.3e}\\n\\\n",
    "Accuracy:          {test_accuracy:.3f}\\n\\\n",
    "Precision:         {test_precision:.3f}\\n\\\n",
    "Recall:            {test_recall:.3f}\\n\\\n",
    "F1 score:          {test_f1_score:.3f}\\n\\\n",
    "CSI score:         {test_csi_score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZCm92IhLeZtD"
   },
   "outputs": [],
   "source": [
    "# test the max recall model - average loss and metrics\n",
    "\n",
    "model_recall = copy.deepcopy(best_model_recall)\n",
    "test_loss, test_accuracy, test_precision, test_recall, test_f1_score, test_csi_score = validation_unet(model_recall, test_loader, device=device, loss_f = loss_f)\n",
    "\n",
    "print(f'Average metrics for test dataset using model with best validation recall:\\n\\n\\\n",
    "{loss_f} loss:          {test_loss:.3e}\\n\\\n",
    "Accuracy:          {test_accuracy:.3f}\\n\\\n",
    "Precision:         {test_precision:.3f}\\n\\\n",
    "Recall:            {test_recall:.3f}\\n\\\n",
    "F1 score:          {test_f1_score:.3f}\\n\\\n",
    "CSI score:         {test_csi_score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D6iEkzwAeZtD"
   },
   "outputs": [],
   "source": [
    "plot_losses_metrics(train_losses, val_losses, metrics, model_recall, loss_f=loss_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLvt8a9peZtE"
   },
   "outputs": [],
   "source": [
    "show_evolution(18, test_set, model_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scDYndvdeZtF"
   },
   "outputs": [],
   "source": [
    "show_evolution(18, test_set, model_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cQ-1S42yeZtF"
   },
   "outputs": [],
   "source": [
    "show_evolution(18, val_set, model_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hO6U2eQBeZtG"
   },
   "outputs": [],
   "source": [
    "single_roc_curve(model_loss, test_set, sample=18, device=device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-A-b1-zfeZtG"
   },
   "outputs": [],
   "source": [
    "single_roc_curve(model_recall, test_set, sample=18, device=device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_yRuFdikeZtH"
   },
   "outputs": [],
   "source": [
    "get_total_roc_curve(model_loss, test_set, device=device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UmfhOjkveZtI"
   },
   "outputs": [],
   "source": [
    "get_total_roc_curve(model_recall, test_set, device=device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WoRvue6deZtI"
   },
   "outputs": [],
   "source": [
    "single_pr_curve(model_loss, test_set, sample=19, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tnRC60iYeZtJ"
   },
   "outputs": [],
   "source": [
    "show_evolution(18, test_set, model_loss)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "braided",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
