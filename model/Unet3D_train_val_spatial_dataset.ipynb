{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNveEhM2eZsZ"
   },
   "source": [
    "# JamUNet model trained with the spatial dataset - training and validation\n",
    "\n",
    "This notebook was used for training and validating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathi\\Desktop\\TU Delft\\TU Delft year 5\\Data_science\\Morphology_project\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1765273429246,
     "user": {
      "displayName": "Mathias Ruhe",
      "userId": "08308838018676251424"
     },
     "user_tz": -60
    },
    "id": "qi0f51gReZsg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    " # reload modules to avoid restarting the notebook every time these are updated\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "executionInfo": {
     "elapsed": 5735,
     "status": "error",
     "timestamp": 1765273111241,
     "user": {
      "displayName": "Mathias Ruhe",
      "userId": "08308838018676251424"
     },
     "user_tz": -60
    },
    "id": "dJHSJLrkeZsh",
    "outputId": "1a94d5f2-c92b-4cd9-a7fe-d1e6d0141079"
   },
   "outputs": [],
   "source": [
    "# import modules\n",
    "\n",
    "import torch\n",
    "import joblib\n",
    "import copy\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from model.train_eval import *\n",
    "from preprocessing.dataset_generation import create_full_dataset\n",
    "from postprocessing.save_results import *\n",
    "from postprocessing.plot_results import *\n",
    "\n",
    "# enable interactive widgets in Jupyter Notebook\n",
    "%matplotlib inline\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "UOIKPAR6eZsk",
    "outputId": "255faef6-4d9e-4fb8-c8c9-9e2509ec35c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Device Count:  1\n",
      "CUDA Device Name:  Quadro P2000\n",
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# set the device where operations are performed\n",
    "# if only one GPU is present you might need to remove the index \"0\"\n",
    "# torch.device('cuda:0') --> torch.device('cuda') / torch.cuda.get_device_name(0) --> torch.cuda.get_device_name()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(\"CUDA Device Count: \", torch.cuda.device_count())\n",
    "    print(\"CUDA Device Name: \", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "6_tSTlVSeZsm",
    "outputId": "370feb5c-6b75-47d5-88da-1b1e0eb1efad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logical CPU cores: 12\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "num_cpus = os.cpu_count()  # total logical cores\n",
    "print(\"Logical CPU cores:\", num_cpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "JSTcQ3b9eZsn"
   },
   "outputs": [],
   "source": [
    "# set common keys required for functions\n",
    "\n",
    "train = 'training'\n",
    "val = 'validation'\n",
    "test = 'testing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "sGejVd6ieZsp",
    "outputId": "16324d00-8dab-44f0-e3c7-afa7a2ad608a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\mathi\\Desktop\\TU Delft\\TU Delft year 5\\Data_science\\Morphology_project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NcjI_TjbeZsr",
    "outputId": "0e7efc2d-dd31-4c2a-fba9-89bb90744dfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: c:\\Users\\mathi\\Desktop\\TU Delft\\TU Delft year 5\\Data_science\\Morphology_project\\jamunet-morpho-braided\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os\n",
    "print(\"Working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CIh3YPnTeZsu",
    "outputId": "37c76ab8-3d9a-4d96-900e-6ef48d13971e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathi\\miniconda3\\envs\\braided\\lib\\site-packages\\osgeo\\gdal.py:330: FutureWarning: Neither gdal.UseExceptions() nor gdal.DontUseExceptions() has been explicitly called. In GDAL 4.0, exceptions will be enabled by default.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load all datasets\n",
    "\n",
    "# by default March images are used - if another month is used change the number (available months: 1-4)\n",
    "dataset_path = r'data\\satellite\\dataset_month3'\n",
    "\n",
    "dtype=torch.float32\n",
    "\n",
    "train_set = create_full_dataset(train, dir_folders=dataset_path, device=device, dtype=dtype)\n",
    "val_set = create_full_dataset(val, dir_folders=dataset_path, device=device, dtype=dtype)\n",
    "test_set = create_full_dataset(test, dir_folders=dataset_path, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9jSA2JDZeZsw",
    "outputId": "e3b0d79d-b359-4778-db90-3a72694c0dce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset samples: 546,\n",
      "Validation dataset samples: 20,\n",
      "Testing dataset samples: 20\n"
     ]
    }
   ],
   "source": [
    "print(f'Training dataset samples: {len(train_set)},\\n\\\n",
    "Validation dataset samples: {len(val_set)},\\n\\\n",
    "Testing dataset samples: {len(test_set)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cR0CKQUseZsx"
   },
   "source": [
    "**<span style=\"color:red\">Attention!</span>**\n",
    "\\\n",
    "Uncomment the next cells if larger training, validation, and testing datasets are needed. These cells load all months datasets (January, February, March, and April) and then merge them into one dataset.\n",
    "\\\n",
    "Keep in mind that due to memory constraints, it is likely that not all four datasets can be loaded.\n",
    "\\\n",
    "Make sure to load the training, validation, and testing datasets in different cells to reduce memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3axPWqKeZsy"
   },
   "outputs": [],
   "source": [
    "# dataset_jan = r'data\\satellite\\dataset_month1'\n",
    "# dataset_feb = r'data\\satellite\\dataset_month2'\n",
    "# dataset_mar = r'data\\satellite\\dataset_month3'\n",
    "# dataset_apr = r'data\\satellite\\dataset_month4'\n",
    "\n",
    "# dtype=torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bb6PlXx2eZsz"
   },
   "outputs": [],
   "source": [
    "# inputs_train_jan, targets_train_jan = create_full_dataset(train, dir_folders=dataset_jan, device=device, dtype=dtype).tensors\n",
    "# inputs_train_feb, targets_train_feb = create_full_dataset(train, dir_folders=dataset_feb, device=device, dtype=dtype).tensors\n",
    "# inputs_train_mar, targets_train_mar = create_full_dataset(train, dir_folders=dataset_mar, device=device, dtype=dtype).tensors\n",
    "# inputs_train_apr, targets_train_apr = create_full_dataset(train, dir_folders=dataset_apr, device=device, dtype=dtype).tensors\n",
    "\n",
    "# inputs_train = torch.cat((inputs_train_jan, inputs_train_feb, inputs_train_mar, inputs_train_apr))\n",
    "# targets_train = torch.cat((targets_train_jan, targets_train_feb, targets_train_mar, targets_train_apr))\n",
    "# train_set = TensorDataset(inputs_train, targets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VSq1Ko5ceZs0"
   },
   "outputs": [],
   "source": [
    "# inputs_val_jan, targets_val_jan = create_full_dataset(val, dir_folders=dataset_jan, device=device, dtype=dtype).tensors\n",
    "# inputs_val_feb, targets_val_feb = create_full_dataset(val, dir_folders=dataset_feb, device=device, dtype=dtype).tensors\n",
    "# inputs_val_mar, targets_val_mar = create_full_dataset(val, dir_folders=dataset_mar, device=device, dtype=dtype).tensors\n",
    "# inputs_val_apr, targets_val_apr = create_full_dataset(val, dir_folders=dataset_apr, device=device, dtype=dtype).tensors\n",
    "\n",
    "# inputs_val = torch.cat((inputs_val_jan, inputs_val_feb, inputs_val_mar, inputs_val_apr))\n",
    "# targets_val = torch.cat((targets_val_jan, targets_val_feb, targets_val_mar, targets_val_apr))\n",
    "# val_set = TensorDataset(inputs_val, targets_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OagE3pHPeZs1"
   },
   "outputs": [],
   "source": [
    "# inputs_test_jan, targets_test_jan = create_full_dataset(test, dir_folders=dataset_jan, device=device, dtype=dtype).tensors\n",
    "# inputs_test_feb, targets_test_feb = create_full_dataset(test, dir_folders=dataset_feb, device=device, dtype=dtype).tensors\n",
    "# inputs_test_mar, targets_test_mar = create_full_dataset(test, dir_folders=dataset_mar, device=device, dtype=dtype).tensors\n",
    "# inputs_test_apr, targets_test_apr = create_full_dataset(test, dir_folders=dataset_apr, device=device, dtype=dtype).tensors\n",
    "\n",
    "# inputs_test = torch.cat((inputs_test_jan, inputs_test_feb, inputs_test_mar, inputs_test_apr))\n",
    "# targets_test = torch.cat((targets_test_jan, targets_test_feb, targets_test_mar, targets_test_apr))\n",
    "# test_set = TensorDataset(inputs_test, targets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsC1zKs1eZs2"
   },
   "source": [
    "**<span style=\"color:red\">Attention!</span>**\n",
    "\\\n",
    "It is not needed to scale and normalize the dataset as the pixel values are already $[0, 1]$.\n",
    "\\\n",
    "If scaling and normalization are performed anyways, then **the model inputs have to be changed** as the normalized datasets are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQSQywxZeZs3"
   },
   "outputs": [],
   "source": [
    "# normalize inputs and targets using the training dataset\n",
    "\n",
    "# scaler_x, scaler_y = scaler(train_set)\n",
    "\n",
    "# normalized_train_set = normalize_dataset(train_set, scaler_x, scaler_y)\n",
    "# normalized_val_set = normalize_dataset(val_set, scaler_x, scaler_y)\n",
    "# normalized_test_set = normalize_dataset(test_set, scaler_x, scaler_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r8uGh0ENeZs3"
   },
   "outputs": [],
   "source": [
    "# save scalers to be loaded in seperate notebooks (i.e., for testing the model)\n",
    "# should not change unless seed is changed or augmentation increased (randomsplit changes)\n",
    "\n",
    "# joblib.dump(scaler_x, r'model\\scalers\\scaler_x.joblib')\n",
    "# joblib.dump(scaler_y, r'model\\scalers\\scaler_y.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tI5dXulleZs4"
   },
   "outputs": [],
   "source": [
    "# load JamUNet architecture\n",
    "\n",
    "from model.st_unet.st_unet import *\n",
    "\n",
    "n_channels = train_set[0][0].shape[0]\n",
    "n_classes = 1\n",
    "init_hid_dim = 8\n",
    "kernel_size = 3\n",
    "pooling = 'max'\n",
    "\n",
    "model = UNet3D(n_channels=n_channels, n_classes=n_classes, init_hid_dim=init_hid_dim,\n",
    "               kernel_size=kernel_size, pooling=pooling, bilinear=False, drop_channels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_OydeTQGeZs4",
    "outputId": "551a098e-58b0-434f-fdd5-4a3bd0ead783"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet3D(\n",
       "  (inc): DoubleConv(\n",
       "    (double_conv): Sequential(\n",
       "      (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv3d): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "    (bn3d): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (down1): Down(\n",
       "    (pooling): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (pool_conv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3d): Conv3d(16, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "        (bn3d): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down2): Down(\n",
       "    (pooling): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (pool_conv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3d): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "        (bn3d): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down3): Down(\n",
       "    (pooling): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (pool_conv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3d): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "        (bn3d): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (temporal_conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "  (down4): Down(\n",
       "    (pooling): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (pool_conv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv3d): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "        (bn3d): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up1): Up(\n",
       "    (up): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv3d): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "      (bn3d): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (up2): Up(\n",
       "    (up): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv3d): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "      (bn3d): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (up3): Up(\n",
       "    (up): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv3d): Conv3d(16, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "      (bn3d): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (up4): Up(\n",
       "    (up): ConvTranspose2d(16, 8, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv3d): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "      (bn3d): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (outc): OutConv(\n",
       "    (conv): Conv2d(8, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print model architecture\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IDDXcZQeeZs5",
    "outputId": "cbe96de2-3a28-41e8-cbd7-329002ba33c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 5.24e+05\n",
      "Model size: 2.00 MB\n"
     ]
    }
   ],
   "source": [
    "# print total number of parameters and model size\n",
    "\n",
    "num_parameters = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {num_parameters:.2e}\")\n",
    "model_size_MB = num_parameters * 4 / (1024 ** 2)  # assuming float32 precision\n",
    "print(f\"Model size: {model_size_MB:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JG6497ReZs6"
   },
   "source": [
    "**<span style=\"color:red\">Attention!</span>**\n",
    "\\\n",
    "Since it is not needed to scale and normalize the dataset (see above), the input for the Data Loader are not the normalized datasets.\n",
    "\\\n",
    "If normalization is performed, the normalized datasets become the inputs to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f6TmF-HZeZs7"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "learning_rate = 0.05\n",
    "batch_size = 16\n",
    "num_epochs = 100\n",
    "water_threshold = 0.5\n",
    "physics = False    # no physics-induced loss terms in the training loss if False\n",
    "alpha_er = 1e-4    # needed only if physics=True\n",
    "alpha_dep = 1e-4   # needed only if physics=True\n",
    "\n",
    "# optimizer to train the model with backpropagation\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "# scheduler for decreasing the learning rate\n",
    "# every tot epochs (step_size) with given factor (gamma)\n",
    "step_size = 15     # set to None to remove the scheduler\n",
    "gamma = 0.75       # set to None to remove the scheduler\n",
    "if (step_size and gamma) is not None:\n",
    "    scheduler = StepLR(optimizer, step_size = step_size, gamma = gamma)\n",
    "\n",
    "# dataloaders to input data to the model in batches -- see note above if normalization is performed\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DWk-h6XoeZs7",
    "outputId": "86477ffe-21f7-47a9-b3b0-8d63966b7b6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ GPU test successful: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Test GPU before training\n",
    "try:\n",
    "    test_tensor = torch.randn(2, 2).to(device)\n",
    "    print(f\"✓ GPU test successful: {device}\")\n",
    "    del test_tensor\n",
    "    torch.cuda.empty_cache()\n",
    "except Exception as e:\n",
    "    print(f\"✗ GPU test failed: {e}\")\n",
    "    print(\"Switching to CPU...\")\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oa-V-WKteZs-",
    "outputId": "f9d10261-8595-468e-bb4e-84f56ea35bad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Training loss: 2.57e-01, Validation loss: 1.93e-01, Best validation loss: 1.93e-01  | Metrics: Accuracy: 0.920, Precision: 0.601, Recall: 0.674, F1-score: 0.635, CSI-score: 0.466, Best recall: 0.674\n",
      "Current learning rate: 0.05\n",
      "Epoch: 2 | Training loss: 1.77e-01, Validation loss: 1.68e-01, Best validation loss: 1.68e-01  | Metrics: Accuracy: 0.924, Precision: 0.629, Recall: 0.651, F1-score: 0.640, CSI-score: 0.471, Best recall: 0.674\n",
      "Current learning rate: 0.05\n",
      "Epoch: 3 | Training loss: 1.75e-01, Validation loss: 1.61e-01, Best validation loss: 1.61e-01  | Metrics: Accuracy: 0.927, Precision: 0.646, Recall: 0.656, F1-score: 0.651, CSI-score: 0.483, Best recall: 0.674\n",
      "Current learning rate: 0.05\n",
      "Epoch: 4 | Training loss: 1.74e-01, Validation loss: 1.59e-01, Best validation loss: 1.59e-01  | Metrics: Accuracy: 0.928, Precision: 0.683, Recall: 0.583, F1-score: 0.629, CSI-score: 0.459, Best recall: 0.674\n",
      "Current learning rate: 0.05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# update the learning rate\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# model training\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_unet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwater_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwater_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_f\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphysics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mphysics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_er\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha_er\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                           \u001b[49m\u001b[43malpha_dep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha_dep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_er_dep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_er_dep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# model validation\u001b[39;00m\n\u001b[0;32m     23\u001b[0m val_loss, val_accuracy, val_precision, val_recall, val_f1_score, val_csi_score \u001b[38;5;241m=\u001b[39m validation_unet(model, val_loader, \n\u001b[0;32m     24\u001b[0m                                                                                                  device\u001b[38;5;241m=\u001b[39mdevice, loss_f\u001b[38;5;241m=\u001b[39mloss_f, \n\u001b[0;32m     25\u001b[0m                                                                                                  water_threshold\u001b[38;5;241m=\u001b[39mwater_threshold)\n",
      "File \u001b[1;32mc:\\Users\\mathi\\Desktop\\TU Delft\\TU Delft year 5\\Data_science\\Morphology_project\\jamunet-morpho-braided\\model\\train_eval.py:118\u001b[0m, in \u001b[0;36mtraining_unet\u001b[1;34m(model, loader, optimizer, nonwater, water, pixel_size, water_threshold, device, loss_f, physics, alpha_er, alpha_dep, loss_er_dep)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m binary_loss\n\u001b[1;32m--> 118\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach())\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# backpropagate and update weights\u001b[39;00m\n\u001b[0;32m    121\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)   \u001b[38;5;66;03m# reset the computed gradients\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initialize training, validation losses and metrics\n",
    "train_losses, val_losses = [], []\n",
    "accuracies, precisions, recalls, f1_scores, csi_scores = [], [], [], [], []\n",
    "\n",
    "# set classification loss - possible options: 'BCE', 'BCE_Logits', and 'Focal'\n",
    "loss_f = 'BCE'\n",
    "# set regression loss for physics-induced terms\n",
    "# possible options: 'Huber', 'RMSE', and 'MAE'\n",
    "loss_er_dep = 'Huber'\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "\n",
    "    # update learning rate\n",
    "    if (step_size and gamma) is not None:\n",
    "        scheduler.step() # update the learning rate\n",
    "\n",
    "    # model training\n",
    "    train_loss = training_unet(model, train_loader, optimizer, water_threshold=water_threshold,\n",
    "                               device=device, loss_f=loss_f, physics=physics, alpha_er=alpha_er,\n",
    "                               alpha_dep=alpha_dep, loss_er_dep=loss_er_dep)\n",
    "\n",
    "    # model validation\n",
    "    val_loss, val_accuracy, val_precision, val_recall, val_f1_score, val_csi_score = validation_unet(model, val_loader,\n",
    "                                                                                                     device=device, loss_f=loss_f,\n",
    "                                                                                                     water_threshold=water_threshold)\n",
    "\n",
    "    if epoch == 1:\n",
    "        best_loss = val_loss\n",
    "        best_recall = val_recall\n",
    "\n",
    "    # save model with min val loss\n",
    "    if val_loss<=best_loss:\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "        count = 0\n",
    "    # save model with max recall\n",
    "    if val_recall>=best_recall:\n",
    "        best_model_recall = copy.deepcopy(model)\n",
    "        best_recall = val_recall\n",
    "        best_epoch = epoch\n",
    "        count = 0\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    accuracies.append(val_accuracy)\n",
    "    precisions.append(val_precision)\n",
    "    recalls.append(val_recall)\n",
    "    f1_scores.append(val_f1_score)\n",
    "    csi_scores.append(val_csi_score)\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    if epoch%1 == 0:\n",
    "        print(f\"Epoch: {epoch} | \" +\n",
    "              f\"Training loss: {train_loss:.2e}, Validation loss: {val_loss:.2e}, Best validation loss: {best_loss:.2e} \" +\n",
    "              f\" | Metrics: Accuracy: {val_accuracy:.3f}, Precision: {val_precision:.3f}, Recall: {val_recall:.3f},\\\n",
    " F1-score: {val_f1_score:.3f}, CSI-score: {val_csi_score:.3f}, Best recall: {best_recall:.3f}\")\n",
    "        if (step_size and gamma) is not None:\n",
    "            print(f'Current learning rate: {scheduler.get_last_lr()[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pBAWRE1zeZs_"
   },
   "outputs": [],
   "source": [
    "metrics = [accuracies, precisions, recalls, f1_scores, csi_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_FkFkSjOeZtA"
   },
   "outputs": [],
   "source": [
    "# store training and validation losses and metrics to be stored in a .csv file for later postprocessing\n",
    "# always check the dataset month key\n",
    "\n",
    "save_losses_metrics(train_losses, val_losses, metrics, 'spatial', model, 3, init_hid_dim,\n",
    "                    kernel_size, pooling, learning_rate, step_size, gamma, batch_size, num_epochs,\n",
    "                    water_threshold, physics, alpha_er, alpha_dep, dir_output=r'model\\losses_metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yaihUn6keZtA"
   },
   "source": [
    "**<span style=\"color:red\">Attention!</span>**\n",
    "\\\n",
    "Always remember to rename the <code>save_path</code> file before running the whole notebook to avoid overwrting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfeizotReZtA"
   },
   "outputs": [],
   "source": [
    "# save model with min validation loss\n",
    "# always check the dataset month key\n",
    "\n",
    "save_model_path(best_model, 'spatial', 'loss', 3, init_hid_dim, kernel_size, pooling, learning_rate,\n",
    "                step_size, gamma, batch_size, num_epochs, water_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g1qP_5oneZtB"
   },
   "outputs": [],
   "source": [
    "# save model with max recall\n",
    "# always check the dataset month key\n",
    "\n",
    "save_model_path(best_model_recall, 'spatial', 'recall', 3, init_hid_dim, kernel_size, pooling, learning_rate,\n",
    "                step_size, gamma, batch_size, num_epochs, water_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x_KmUfhweZtC"
   },
   "outputs": [],
   "source": [
    "# test the min loss model - average loss and metrics\n",
    "\n",
    "model_loss = copy.deepcopy(best_model)\n",
    "test_loss, test_accuracy, test_precision, test_recall, test_f1_score, test_csi_score = validation_unet(model_loss, test_loader, device=device, loss_f = loss_f)\n",
    "\n",
    "print(f'Average metrics for test dataset using model with best validation loss:\\n\\n\\\n",
    "{loss_f} loss:          {test_loss:.3e}\\n\\\n",
    "Accuracy:          {test_accuracy:.3f}\\n\\\n",
    "Precision:         {test_precision:.3f}\\n\\\n",
    "Recall:            {test_recall:.3f}\\n\\\n",
    "F1 score:          {test_f1_score:.3f}\\n\\\n",
    "CSI score:         {test_csi_score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZCm92IhLeZtD"
   },
   "outputs": [],
   "source": [
    "# test the max recall model - average loss and metrics\n",
    "\n",
    "model_recall = copy.deepcopy(best_model_recall)\n",
    "test_loss, test_accuracy, test_precision, test_recall, test_f1_score, test_csi_score = validation_unet(model_recall, test_loader, device=device, loss_f = loss_f)\n",
    "\n",
    "print(f'Average metrics for test dataset using model with best validation recall:\\n\\n\\\n",
    "{loss_f} loss:          {test_loss:.3e}\\n\\\n",
    "Accuracy:          {test_accuracy:.3f}\\n\\\n",
    "Precision:         {test_precision:.3f}\\n\\\n",
    "Recall:            {test_recall:.3f}\\n\\\n",
    "F1 score:          {test_f1_score:.3f}\\n\\\n",
    "CSI score:         {test_csi_score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D6iEkzwAeZtD"
   },
   "outputs": [],
   "source": [
    "plot_losses_metrics(train_losses, val_losses, metrics, model_recall, loss_f=loss_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLvt8a9peZtE"
   },
   "outputs": [],
   "source": [
    "show_evolution(18, test_set, model_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scDYndvdeZtF"
   },
   "outputs": [],
   "source": [
    "show_evolution(18, test_set, model_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cQ-1S42yeZtF"
   },
   "outputs": [],
   "source": [
    "show_evolution(18, val_set, model_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hO6U2eQBeZtG"
   },
   "outputs": [],
   "source": [
    "single_roc_curve(model_loss, test_set, sample=18, device=device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-A-b1-zfeZtG"
   },
   "outputs": [],
   "source": [
    "single_roc_curve(model_recall, test_set, sample=18, device=device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_yRuFdikeZtH"
   },
   "outputs": [],
   "source": [
    "get_total_roc_curve(model_loss, test_set, device=device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UmfhOjkveZtI"
   },
   "outputs": [],
   "source": [
    "get_total_roc_curve(model_recall, test_set, device=device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WoRvue6deZtI"
   },
   "outputs": [],
   "source": [
    "single_pr_curve(model_loss, test_set, sample=19, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tnRC60iYeZtJ"
   },
   "outputs": [],
   "source": [
    "show_evolution(18, test_set, model_loss)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "braided",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
