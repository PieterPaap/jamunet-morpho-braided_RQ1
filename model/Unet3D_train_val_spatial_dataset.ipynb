{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JamUNet: Multi-River Training & Validation\n",
    "\n",
    "This notebook handles the training and validation of the JamUNet model. To ensure the model generalizes across braided river morphologies while maintaining high accuracy on the target reach, we utilize a multi-domain dataset encompassing the Jamuna, Ganges, Indus, and Ghangara rivers. In case you want to add any new rivers, make sure to execute the following preliminary scripts for all rivers to ensure the data is standardized and the temporal sequences are correctly structured:\n",
    "\n",
    "- `preliminary\\edit_satellite_img.ipynb`: Handles coordinate alignment, remapping pixel classes (Water, Land, No-data), and standardized cropping.\n",
    "\n",
    "- `preliminary\\create_dataset_monthly.ipynb`: Groups images into monthly bins and generates the seasonal averages required for \"No-Data\" gap filling.\n",
    "\n",
    "The notenbook is built around the st_unet3D model and training, validation and test loops which can be found in:\n",
    "\n",
    "- `model\\st_unet\\st_unet_3D.py`\n",
    "\n",
    "- `model\\train_eval.py` \n",
    "\n",
    "**Training Strategy & Data Integration**\n",
    "\n",
    "The dataset is integrated using a \"Domain Adaptation\" approach to optimize performance specifically for the Jamuna River:\n",
    "\n",
    "- Spatial Aggregation: Reaches from the Ganges, Indus, and Ghangara rivers are added to the training set to provide a large, diverse set of braiding patterns.\n",
    "\n",
    "- Targeted Validation: The Jamuna River is partitioned into distinct spatial reaches. Validation and Testing sets consist exclusively of Jamuna data to ensure the performance metrics reflect the model's accuracy on the specific target domain.\n",
    "\n",
    "- Importance Sampling: A WeightedRandomSampler is utilized in the Training DataLoader. This ensures the model encounters Jamuna training samples more frequently than samples from the auxiliary rivers, biasing the U-Net weights toward Jamuna-specific morphology.\n",
    "\n",
    "**Hardware Management**\n",
    "\n",
    "- CPU Loading: Datasets are kept on CPU RAM to manage the large memory footprint of multi-river imagery.\n",
    "\n",
    "- Batch-wise GPU Transfer: To prevent GPU memory errors, data is moved to the GPU device in small, controlled batches during the forward and backward passes.\n",
    "\n",
    "## Step 1: Set up\n",
    "Setting up working directory, environment and checking available GPUn and CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathi\\Desktop\\TU Delft\\TU Delft year 5\\Data_science\\Morphology_project\\jamunet-morpho-braided\n"
     ]
    }
   ],
   "source": [
    "%cd c:\\Users\\mathi\\Desktop\\TU Delft\\TU Delft year 5\\Data_science\\Morphology_project\\jamunet-morpho-braided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "executionInfo": {
     "elapsed": 5735,
     "status": "error",
     "timestamp": 1765273111241,
     "user": {
      "displayName": "Mathias Ruhe",
      "userId": "08308838018676251424"
     },
     "user_tz": -60
    },
    "id": "dJHSJLrkeZsh",
    "outputId": "1a94d5f2-c92b-4cd9-a7fe-d1e6d0141079"
   },
   "outputs": [],
   "source": [
    "# import modules\n",
    "import os\n",
    "import torch\n",
    "import joblib\n",
    "import copy\n",
    "import warnings\n",
    "from torch.utils.data import DataLoader, TensorDataset, ConcatDataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from model.train_eval import *\n",
    "from preprocessing.dataset_generation_modified import create_full_dataset\n",
    "from postprocessing.save_results import *\n",
    "from postprocessing.plot_results import *\n",
    "from osgeo import gdal\n",
    "\n",
    "## enable interactive widgets in Jupyter Notebook\n",
    "%matplotlib inline\n",
    "%matplotlib widget\n",
    "\n",
    "### reload modules to avoid restarting the notebook every time these are updated\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.join('data', 'satellite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UOIKPAR6eZsk",
    "outputId": "255faef6-4d9e-4fb8-c8c9-9e2509ec35c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Device Count:  1\n",
      "CUDA Device Name:  Quadro P2000\n",
      "Using device: cuda:0\n",
      "\n",
      "Logical CPU cores: 12\n"
     ]
    }
   ],
   "source": [
    "# set the device where operations are performed\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(\"CUDA Device Count: \", torch.cuda.device_count())\n",
    "    print(\"CUDA Device Name: \", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "num_cpus = os.cpu_count()  # total logical cores\n",
    "print(\"\\nLogical CPU cores:\", num_cpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set up samples\n",
    "\n",
    "The code bellow loops over the available rivers and the preprocessed samples in each one of them. Since the structure of the Jamuna river and the other rivers differs slightly, the loop looks over several options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Datasets for MONTH 3...\n",
      "\n",
      "River: Jamuna\n",
      "    Reading from: data\\satellite\\Jamuna_images\\dataset_month3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathi\\miniconda3\\envs\\braided_test\\lib\\site-packages\\osgeo\\gdal.py:330: FutureWarning: Neither gdal.UseExceptions() nor gdal.DontUseExceptions() has been explicitly called. In GDAL 4.0, exceptions will be enabled by default.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "River: Ganges\n",
      "    Reading from: data\\satellite\\Ganges_images\\preprocessed\\month_3\n",
      "River: Indus\n",
      "    Reading from: data\\satellite\\Indus_images\\preprocessed\\month_3\n",
      "River: Ghangara\n",
      "    Reading from: data\\satellite\\Ghangara_images\\preprocessed\\month_3\n",
      "\n",
      "========================================\n",
      "FINAL SUMMARY (Month 3)\n",
      "========================================\n",
      "Training Set:   791 samples\n",
      "Validation Set: 14 samples\n",
      "Testing Set:    18 samples\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# Initiate dataset building\n",
    "# define parameters\n",
    "\n",
    "target_month = 3\n",
    "device = 'cpu' # Keep data on CPU\n",
    "dtype = torch.float32\n",
    "\n",
    "train_lists, val_lists, test_lists = [], [], []\n",
    "\n",
    "print(f\"Building Datasets for MONTH {target_month}...\\n\")\n",
    "\n",
    "rivers = ['Jamuna', 'Ganges', 'Indus', 'Ghangara']\n",
    "\n",
    "for river in rivers:\n",
    "    print(f\"River: {river}\")\n",
    "    \n",
    "    # Intelligent Path Finder\n",
    "    possible_paths = [\n",
    "        os.path.join(base_dir, f'{river}_images', f'dataset_month{target_month}'),\n",
    "        os.path.join(base_dir, f'{river}_images', f'dataset_month_{target_month}'),\n",
    "        os.path.join(base_dir, f'{river}_images', 'preprocessed', f'month_{target_month}'),\n",
    "        os.path.join(base_dir, f'{river}_images', 'preprocessed', f'month{target_month}')\n",
    "    ]\n",
    "    \n",
    "    source_path = None\n",
    "    for p in possible_paths:\n",
    "        if os.path.exists(p):\n",
    "            source_path = p\n",
    "            break\n",
    "            \n",
    "    if source_path is None:\n",
    "        print(f\"    SKIPPING: Could not find month folder.\")\n",
    "        continue\n",
    "\n",
    "    # 1. GENERATE DATASETS\n",
    "    if river == 'Jamuna':\n",
    "        # Use specific filter for Jamuna to avoid mixing val/test into training\n",
    "        ds_tr = create_full_dataset('training', dir_folders=source_path, name_filter='training', device=device, dtype=dtype)\n",
    "        \n",
    "        # Pull Val and Test only for Jamuna\n",
    "        ds_val = create_full_dataset('validation', dir_folders=source_path, name_filter='validation', device=device, dtype=dtype)\n",
    "        if len(ds_val) > 0: val_lists.append(ds_val)\n",
    "    \n",
    "        ds_test = create_full_dataset('testing', dir_folders=source_path, name_filter='testing', device=device, dtype=dtype)\n",
    "        if len(ds_test) > 0: test_lists.append(ds_test)\n",
    "    else:\n",
    "        # For other rivers without 'training' tags, name_filter=None is correct\n",
    "        ds_tr = create_full_dataset('training', dir_folders=source_path, name_filter=None, device=device, dtype=dtype)\n",
    "\n",
    "    # 2. SINGLE APPEND (This handles all rivers correctly)\n",
    "    if ds_tr and len(ds_tr) > 0:\n",
    "        train_lists.append(ds_tr)\n",
    "        print(f\"    Added {len(ds_tr)} training samples from {river}.\")\n",
    "\n",
    "# Final Merge\n",
    "final_train_set = ConcatDataset(train_lists) if train_lists else None\n",
    "final_val_set = ConcatDataset(val_lists) if val_lists else None\n",
    "final_test_set = ConcatDataset(test_lists) if test_lists else None\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"FINAL SUMMARY (Month {target_month})\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Training Set:   {len(final_train_set) if final_train_set else 0} samples\")\n",
    "print(f\"Validation Set: {len(final_val_set) if final_val_set else 0} samples\")\n",
    "print(f\"Testing Set:    {len(final_test_set) if final_test_set else 0} samples\")\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cR0CKQUseZsx"
   },
   "source": [
    "**<span style=\"color:red\">Attention!</span>**\n",
    "\\\n",
    "Uncomment the next cells if larger training, validation, and testing datasets are needed. These cells load all months datasets (January, February, March, and April) and then merge them into one dataset.\n",
    "\\\n",
    "Keep in mind that due to memory constraints, it is likely that not all four datasets can be loaded.\n",
    "\\\n",
    "Make sure to load the training, validation, and testing datasets in different cells to reduce memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "v3axPWqKeZsy"
   },
   "outputs": [],
   "source": [
    "# dataset_jan = r'data\\satellite\\dataset_month1'\n",
    "# dataset_feb = r'data\\satellite\\dataset_month2'\n",
    "# dataset_mar = r'data\\satellite\\dataset_month3'\n",
    "# dataset_apr = r'data\\satellite\\dataset_month4'\n",
    "\n",
    "# dtype=torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bb6PlXx2eZsz"
   },
   "outputs": [],
   "source": [
    "# inputs_train_jan, targets_train_jan = create_full_dataset(train, dir_folders=dataset_jan, device=device, dtype=dtype).tensors\n",
    "# inputs_train_feb, targets_train_feb = create_full_dataset(train, dir_folders=dataset_feb, device=device, dtype=dtype).tensors\n",
    "# inputs_train_mar, targets_train_mar = create_full_dataset(train, dir_folders=dataset_mar, device=device, dtype=dtype).tensors\n",
    "# inputs_train_apr, targets_train_apr = create_full_dataset(train, dir_folders=dataset_apr, device=device, dtype=dtype).tensors\n",
    "\n",
    "# inputs_train = torch.cat((inputs_train_jan, inputs_train_feb, inputs_train_mar, inputs_train_apr))\n",
    "# targets_train = torch.cat((targets_train_jan, targets_train_feb, targets_train_mar, targets_train_apr))\n",
    "# train_set = TensorDataset(inputs_train, targets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VSq1Ko5ceZs0"
   },
   "outputs": [],
   "source": [
    "# inputs_val_jan, targets_val_jan = create_full_dataset(val, dir_folders=dataset_jan, device=device, dtype=dtype).tensors\n",
    "# inputs_val_feb, targets_val_feb = create_full_dataset(val, dir_folders=dataset_feb, device=device, dtype=dtype).tensors\n",
    "# inputs_val_mar, targets_val_mar = create_full_dataset(val, dir_folders=dataset_mar, device=device, dtype=dtype).tensors\n",
    "# inputs_val_apr, targets_val_apr = create_full_dataset(val, dir_folders=dataset_apr, device=device, dtype=dtype).tensors\n",
    "\n",
    "# inputs_val = torch.cat((inputs_val_jan, inputs_val_feb, inputs_val_mar, inputs_val_apr))\n",
    "# targets_val = torch.cat((targets_val_jan, targets_val_feb, targets_val_mar, targets_val_apr))\n",
    "# val_set = TensorDataset(inputs_val, targets_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "OagE3pHPeZs1"
   },
   "outputs": [],
   "source": [
    "# inputs_test_jan, targets_test_jan = create_full_dataset(test, dir_folders=dataset_jan, device=device, dtype=dtype).tensors\n",
    "# inputs_test_feb, targets_test_feb = create_full_dataset(test, dir_folders=dataset_feb, device=device, dtype=dtype).tensors\n",
    "# inputs_test_mar, targets_test_mar = create_full_dataset(test, dir_folders=dataset_mar, device=device, dtype=dtype).tensors\n",
    "# inputs_test_apr, targets_test_apr = create_full_dataset(test, dir_folders=dataset_apr, device=device, dtype=dtype).tensors\n",
    "\n",
    "# inputs_test = torch.cat((inputs_test_jan, inputs_test_feb, inputs_test_mar, inputs_test_apr))\n",
    "# targets_test = torch.cat((targets_test_jan, targets_test_feb, targets_test_mar, targets_test_apr))\n",
    "# test_set = TensorDataset(inputs_test, targets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsC1zKs1eZs2"
   },
   "source": [
    "**<span style=\"color:red\">Attention!</span>**\n",
    "\\\n",
    "It is not needed to scale and normalize the dataset as the pixel values are already $[0, 1]$.\n",
    "\\\n",
    "If scaling and normalization are performed anyways, then **the model inputs have to be changed** as the normalized datasets are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "RQSQywxZeZs3"
   },
   "outputs": [],
   "source": [
    "# normalize inputs and targets using the training dataset\n",
    "\n",
    "# scaler_x, scaler_y = scaler(train_set)\n",
    "\n",
    "# normalized_train_set = normalize_dataset(train_set, scaler_x, scaler_y)\n",
    "# normalized_val_set = normalize_dataset(val_set, scaler_x, scaler_y)\n",
    "# normalized_test_set = normalize_dataset(test_set, scaler_x, scaler_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "r8uGh0ENeZs3"
   },
   "outputs": [],
   "source": [
    "# save scalers to be loaded in seperate notebooks (i.e., for testing the model)\n",
    "# should not change unless seed is changed or augmentation increased (randomsplit changes)\n",
    "\n",
    "# joblib.dump(scaler_x, r'model\\scalers\\scaler_x.joblib')\n",
    "# joblib.dump(scaler_y, r'model\\scalers\\scaler_y.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Set up model\n",
    "\n",
    "Load one of the different models to use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "tI5dXulleZs4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet3D(\n",
      "  (inc): DoubleConv(\n",
      "    (net): Sequential(\n",
      "      (0): Conv3d(1, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv3d(8, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (4): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (down1): Down(\n",
      "    (net): Sequential(\n",
      "      (0): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): DoubleConv(\n",
      "        (net): Sequential(\n",
      "          (0): Conv3d(8, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (4): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (down2): Down(\n",
      "    (net): Sequential(\n",
      "      (0): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): DoubleConv(\n",
      "        (net): Sequential(\n",
      "          (0): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (4): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (down3): Down(\n",
      "    (net): Sequential(\n",
      "      (0): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): DoubleConv(\n",
      "        (net): Sequential(\n",
      "          (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (down4): Down(\n",
      "    (net): Sequential(\n",
      "      (0): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): DoubleConv(\n",
      "        (net): Sequential(\n",
      "          (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (4): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (temporal): Sequential(\n",
      "    (0): Conv3d(128, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "    (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv3d(128, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
      "    (4): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (up1): Up(\n",
      "    (up): ConvTranspose3d(128, 64, kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
      "    (conv): DoubleConv(\n",
      "      (net): Sequential(\n",
      "        (0): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up2): Up(\n",
      "    (up): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
      "    (conv): DoubleConv(\n",
      "      (net): Sequential(\n",
      "        (0): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (4): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up3): Up(\n",
      "    (up): ConvTranspose3d(32, 16, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "    (conv): DoubleConv(\n",
      "      (net): Sequential(\n",
      "        (0): Conv3d(32, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (4): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up4): Up(\n",
      "    (up): ConvTranspose3d(16, 8, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "    (conv): DoubleConv(\n",
      "      (net): Sequential(\n",
      "        (0): Conv3d(16, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv3d(8, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (4): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (outc): OutConv(\n",
      "    (conv): Conv3d(8, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "  )\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Number of parameters: 1.47e+06\n",
      "Model size: 5.61 MB\n"
     ]
    }
   ],
   "source": [
    "# load JamUNet architecture\n",
    "\n",
    "from model.st_unet.st_unet_3D import *\n",
    "\n",
    "n_channels = final_train_set[0][0].shape[0]\n",
    "n_classes = 1\n",
    "init_hid_dim = 8\n",
    "kernel_size = 3\n",
    "pooling = 'max'\n",
    "\n",
    "model = UNet3D(n_channels=n_channels, n_classes=n_classes, init_hid_dim=init_hid_dim,\n",
    "               kernel_size=kernel_size, pooling=pooling, bilinear=False, drop_channels=False)\n",
    "\n",
    "# print model architecture\n",
    "\n",
    "print(model)\n",
    "\n",
    "# print total number of parameters and model size\n",
    "\n",
    "num_parameters = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {num_parameters:.2e}\")\n",
    "model_size_MB = num_parameters * 4 / (1024 ** 2)  # assuming float32 precision\n",
    "print(f\"Model size: {model_size_MB:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JG6497ReZs6"
   },
   "source": [
    "**<span style=\"color:red\">Attention!</span>**\n",
    "\\\n",
    "Since it is not needed to scale and normalize the dataset (see above), the input for the Data Loader are not the normalized datasets.\n",
    "\\\n",
    "If normalization is performed, the normalized datasets become the inputs to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1: Defining hyperparameters and running Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "f6TmF-HZeZs7"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "learning_rate = 0.05\n",
    "batch_size = 16\n",
    "num_epochs = 1\n",
    "water_threshold = 0.5\n",
    "physics = False    # no physics-induced loss terms in the training loss if False\n",
    "alpha_er = 1e-4    # needed only if physics=True\n",
    "alpha_dep = 1e-4   # needed only if physics=True\n",
    "\n",
    "# optimizer to train the model with backpropagation\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "# scheduler for decreasing the learning rate\n",
    "# every tot epochs (step_size) with given factor (gamma)\n",
    "step_size = 15     # set to None to remove the scheduler\n",
    "gamma = 0.75       # set to None to remove the scheduler\n",
    "if (step_size and gamma) is not None:\n",
    "    scheduler = StepLR(optimizer, step_size = step_size, gamma = gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loader Ready!\n",
      "Test Loader Ready!\n",
      "WeightedRandomSampler initialized successfully.\n",
      "Total training samples: 791\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "batch_size = 16\n",
    "\n",
    "# 1. Define the importance weights for each river\n",
    "# We prioritize Jamuna to ensure the model optimizes for our target domain\n",
    "river_importance = {\n",
    "    'Jamuna': 3.0,\n",
    "    'Ganges': 1.0,\n",
    "    'Indus': 1.0,\n",
    "    'Ghangara': 1.0\n",
    "}\n",
    "\n",
    "# 2. Identify the river order used when building train_lists\n",
    "# This must match the order in your 'rivers' loop\n",
    "rivers_in_order = ['Jamuna', 'Ganges', 'Indus', 'Ghangara']\n",
    "\n",
    "sample_weights = []\n",
    "\n",
    "# 3. Assign the weight to every individual sample in the combined dataset\n",
    "for i, dataset in enumerate(train_lists):\n",
    "    river_name = rivers_in_order[i]\n",
    "    weight = river_importance[river_name]\n",
    "    \n",
    "    # Extend the weight list by the number of samples in this river's dataset\n",
    "    sample_weights.extend([weight] * len(dataset))\n",
    "\n",
    "# 4. Create the Sampler\n",
    "# num_samples=len(sample_weights) ensures the epoch length remains the same\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights, \n",
    "    num_samples=len(sample_weights), \n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# 5. Initialize the Training DataLoader\n",
    "# Note: 'shuffle' must be False when using a sampler\n",
    "train_loader = DataLoader(\n",
    "    final_train_set, \n",
    "    batch_size=batch_size, \n",
    "    sampler=sampler, \n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "# dataloaders to input data to the model in batches -- see note above if normalization is performed\n",
    "# Create DataLoaders\n",
    "if final_val_set and len(final_val_set) > 0:\n",
    "    val_loader = DataLoader(final_val_set, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    print(\"Val Loader Ready!\")\n",
    "if final_test_set and len(final_test_set) > 0:\n",
    "    test_loader = DataLoader(final_test_set, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    print(\"Test Loader Ready!\")\n",
    "\n",
    "print(f\"WeightedRandomSampler initialized successfully.\")\n",
    "print(f\"Total training samples: {len(sample_weights)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Device Count:  1\n",
      "CUDA Device Name:  Quadro P2000\n",
      "Using device: cuda:0\n",
      "\n",
      "Logical CPU cores: 12\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(\"CUDA Device Count: \", torch.cuda.device_count())\n",
    "    print(\"CUDA Device Name: \", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "num_cpus = os.cpu_count()  # total logical cores\n",
    "print(\"\\nLogical CPU cores:\", num_cpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "DWk-h6XoeZs7",
    "outputId": "86477ffe-21f7-47a9-b3b0-8d63966b7b6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ GPU test successful: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Test GPU before training\n",
    "try:\n",
    "    test_tensor = torch.randn(2, 2).to(device)\n",
    "    print(f\"✓ GPU test successful: {device}\")\n",
    "    del test_tensor\n",
    "    torch.cuda.empty_cache()\n",
    "except Exception as e:\n",
    "    print(f\"✗ GPU test failed: {e}\")\n",
    "    print(\"Switching to CPU...\")\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2: Run training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oa-V-WKteZs-",
    "outputId": "f9d10261-8595-468e-bb4e-84f56ea35bad"
   },
   "outputs": [],
   "source": [
    "# initialize training, validation losses and metrics\n",
    "train_losses, val_losses = [], []\n",
    "accuracies, precisions, recalls, f1_scores, csi_scores = [], [], [], [], []\n",
    "count = 0\n",
    "\n",
    "# set classification loss - possible options: 'BCE', 'BCE_Logits', and 'Focal'\n",
    "loss_f = 'BCE'\n",
    "# set regression loss for physics-induced terms\n",
    "# possible options: 'Huber', 'RMSE', and 'MAE'\n",
    "loss_er_dep = 'Huber'\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "\n",
    "    # update learning rate\n",
    "    if (step_size and gamma) is not None:\n",
    "        scheduler.step() # update the learning rate\n",
    "\n",
    "    # model training\n",
    "    train_loss = training_unet(model, train_loader, optimizer, water_threshold=water_threshold,\n",
    "                               device=device, loss_f=loss_f, physics=physics, alpha_er=alpha_er,\n",
    "                               alpha_dep=alpha_dep, loss_er_dep=loss_er_dep)\n",
    "\n",
    "    # model validation\n",
    "    val_loss, val_accuracy, val_precision, val_recall, val_f1_score, val_csi_score = validation_unet(model, val_loader,\n",
    "                                                                                                     device=device, loss_f=loss_f,\n",
    "                                                                                                     water_threshold=water_threshold)\n",
    "\n",
    "    if epoch == 1:\n",
    "        best_loss = val_loss\n",
    "        best_recall = val_recall\n",
    "\n",
    "    # save model with min val loss\n",
    "    if val_loss<=best_loss:\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "        count = 0\n",
    "    # save model with max recall\n",
    "    if val_recall>=best_recall:\n",
    "        best_model_recall = copy.deepcopy(model)\n",
    "        best_recall = val_recall\n",
    "        best_epoch = epoch\n",
    "        count = 0\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    accuracies.append(val_accuracy)\n",
    "    precisions.append(val_precision)\n",
    "    recalls.append(val_recall)\n",
    "    f1_scores.append(val_f1_score)\n",
    "    csi_scores.append(val_csi_score)\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    if epoch%1 == 0:\n",
    "        print(f\"Epoch: {epoch} | \" +\n",
    "              f\"Training loss: {train_loss:.2e}, Validation loss: {val_loss:.2e}, Best validation loss: {best_loss:.2e} \" +\n",
    "              f\" | Metrics: Accuracy: {val_accuracy:.3f}, Precision: {val_precision:.3f}, Recall: {val_recall:.3f},\\\n",
    " F1-score: {val_f1_score:.3f}, CSI-score: {val_csi_score:.3f}, Best recall: {best_recall:.3f}\")\n",
    "        if (step_size and gamma) is not None:\n",
    "            print(f'Current learning rate: {scheduler.get_last_lr()[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pBAWRE1zeZs_"
   },
   "outputs": [],
   "source": [
    "metrics = [accuracies, precisions, recalls, f1_scores, csi_scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.3: Save model losses and model paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_FkFkSjOeZtA"
   },
   "outputs": [],
   "source": [
    "save_losses_metrics(\n",
    "    train_losses, 'Jamuna_Ganges_Ghangara_Indus', # This goes 2nd because it is 2nd in your 'def'\n",
    "    val_losses, metrics, 'spatial', model, 3, \n",
    "    init_hid_dim, kernel_size, pooling, learning_rate, step_size, gamma, batch_size, \n",
    "    num_epochs,water_threshold, physics, alpha_er, \n",
    "    alpha_dep, dir_output='model/losses_metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yaihUn6keZtA"
   },
   "source": [
    "**<span style=\"color:red\">Attention!</span>**\n",
    "\\\n",
    "Always remember to rename the <code>save_path</code> file before running the whole notebook to avoid overwrting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfeizotReZtA"
   },
   "outputs": [],
   "source": [
    "# save model with min validation loss\n",
    "# always check the dataset month key\n",
    "# save model with min validation loss\n",
    "# always check the dataset month key\n",
    "\n",
    "save_model_path(best_model,'Jamuna_Ganges_Ghangara_Indus', 'spatial', 'loss', 3, init_hid_dim, kernel_size, pooling, learning_rate,\n",
    "                step_size, gamma, batch_size, num_epochs, water_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g1qP_5oneZtB"
   },
   "outputs": [],
   "source": [
    "# save model with max recall\n",
    "# always check the dataset month key\n",
    "\n",
    "save_model_path(best_model_recall,'Jamuna_Ganges_Ghangara_Indus', 'spatial', 'recall', 3, init_hid_dim, kernel_size, pooling, learning_rate,\n",
    "                step_size, gamma, batch_size, num_epochs, water_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate model performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x_KmUfhweZtC"
   },
   "outputs": [],
   "source": [
    "# test the min loss model - average loss and metrics\n",
    "\n",
    "model_loss = copy.deepcopy(best_model)\n",
    "test_loss, test_accuracy, test_precision, test_recall, test_f1_score, test_csi_score = validation_unet(model_loss, test_loader, device=device, loss_f = loss_f)\n",
    "\n",
    "print(f'Average metrics for test dataset using model with best validation loss:\\n\\n\\\n",
    "{loss_f} loss:          {test_loss:.3e}\\n\\\n",
    "Accuracy:          {test_accuracy:.3f}\\n\\\n",
    "Precision:         {test_precision:.3f}\\n\\\n",
    "Recall:            {test_recall:.3f}\\n\\\n",
    "F1 score:          {test_f1_score:.3f}\\n\\\n",
    "CSI score:         {test_csi_score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZCm92IhLeZtD"
   },
   "outputs": [],
   "source": [
    "# test the max recall model - average loss and metrics\n",
    "\n",
    "model_recall = copy.deepcopy(best_model_recall)\n",
    "test_loss, test_accuracy, test_precision, test_recall, test_f1_score, test_csi_score = validation_unet(model_recall, test_loader, device=device, loss_f = loss_f)\n",
    "\n",
    "print(f'Average metrics for test dataset using model with best validation recall:\\n\\n\\\n",
    "{loss_f} loss:          {test_loss:.3e}\\n\\\n",
    "Accuracy:          {test_accuracy:.3f}\\n\\\n",
    "Precision:         {test_precision:.3f}\\n\\\n",
    "Recall:            {test_recall:.3f}\\n\\\n",
    "F1 score:          {test_f1_score:.3f}\\n\\\n",
    "CSI score:         {test_csi_score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D6iEkzwAeZtD"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plot_losses_metrics(train_losses, val_losses, metrics, model_recall, loss_f=loss_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLvt8a9peZtE"
   },
   "outputs": [],
   "source": [
    "show_evolution_nolegend_nn(len(final_test_set)-1, final_test_set, model_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scDYndvdeZtF"
   },
   "outputs": [],
   "source": [
    "show_evolution_nolegend_nn(len(final_test_set)-1, final_test_set, model_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cQ-1S42yeZtF"
   },
   "outputs": [],
   "source": [
    "show_evolution_nolegend_nn(len(final_val_set)-1, final_val_set, model_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hO6U2eQBeZtG"
   },
   "outputs": [],
   "source": [
    "single_roc_curve(model_loss, final_test_set, sample=len(final_test_set)-1, device=device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-A-b1-zfeZtG"
   },
   "outputs": [],
   "source": [
    "single_roc_curve(model_recall, final_test_set, sample=len(final_test_set)-1, device=device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_yRuFdikeZtH"
   },
   "outputs": [],
   "source": [
    "get_total_roc_curve(model_loss, final_test_set, device=device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UmfhOjkveZtI"
   },
   "outputs": [],
   "source": [
    "get_total_roc_curve(model_recall, final_test_set, device=device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WoRvue6deZtI"
   },
   "outputs": [],
   "source": [
    "single_pr_curve(model_loss, final_test_set, sample=19, device=device)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "braided_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
