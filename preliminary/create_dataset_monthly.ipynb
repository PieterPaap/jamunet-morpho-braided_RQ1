{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation\n",
    "\n",
    "This notebook is used for creating the datasets used for the training, validation and testing of the deep-learning model. \n",
    "\n",
    "Author of origibal notebook: Antonio Magherini (Antonio.Magherini@deltares.nl).\n",
    "\n",
    "Notebook modified by Mathias Ruhe (mathias.d.ruhe@gmail.com)\n",
    "\n",
    "**Modifications:**\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathi\\Desktop\\TU Delft\\TU Delft year 5\\Data_science\\Morphology_project\\jamunet-morpho-braided\n"
     ]
    }
   ],
   "source": [
    "# move to root directory\n",
    "\n",
    "%cd c:\\Users\\mathi\\Desktop\\TU Delft\\TU Delft year 5\\Data_science\\Morphology_project\\jamunet-morpho-braided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload modules to avoid restarting the notebook every time these are updated\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules \n",
    "\n",
    "import torch \n",
    "\n",
    "from preprocessing.dataset_generation import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## path definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory set to: data\\satellite\\Ganges_images\n"
     ]
    }
   ],
   "source": [
    "# Set river\n",
    "rivers = ['Jamuna','Ganges', 'Indus', 'Ghangara']\n",
    "river = rivers[1]  # change index to select different river\n",
    "\n",
    "# 2. Build the specific paths\n",
    "base_dir = os.path.join('data', 'satellite', f'{river}_images')\n",
    "dir_orig = os.path.join(base_dir, 'original')\n",
    "dir_proc = os.path.join(base_dir, 'preprocessed')\n",
    "dir_dataset = os.path.join(base_dir, 'dataset')\n",
    "print(f'Base directory set to: {base_dir}')\n",
    "\n",
    "# Subdirectories\n",
    "dir_dataset_1024x512 = os.path.join(base_dir, 'dataset_1024x512')\n",
    "dir_dataset_jan = os.path.join(dir_proc, 'month_1')\n",
    "dir_dataset_feb = os.path.join(dir_proc, 'month_2')\n",
    "dir_dataset_mar = os.path.join(dir_proc, 'month_3')\n",
    "dir_dataset_apr = os.path.join(dir_proc, 'month_4')\n",
    "\n",
    "# Available collections\n",
    "JRC = r'JRC_GSW1_4_MonthlyHistory'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set string variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = 'training'\n",
    "val = 'validation'\n",
    "test = 'testing'\n",
    "\n",
    "train_val_test_list = [train, val, test]\n",
    "train_list = [train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cells are used just to show how the different functions work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create the input and target datasets: all images are loaded regardless of their quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_mar, target_mar = create_datasets(train, 1, 5, dir_folders=dir_dataset_mar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from osgeo import gdal\n",
    "\n",
    "# --- Helper Loading Functions (Assumed you have them, provided here for safety) ---\n",
    "def load_image_array(path, scaled_classes=True):\n",
    "    ds = gdal.Open(path)\n",
    "    if ds is None: return None\n",
    "    arr = ds.ReadAsArray()\n",
    "    # If scaled, ensure mapping is correct (-1, 0, 1) or keep raw\n",
    "    return arr\n",
    "\n",
    "def load_avg(train_val_test, reach, year, dir_averages):\n",
    "    # Construct path to the specific average file\n",
    "    # Structure: averages / average_training_r1 / average_1988_training_r1.csv\n",
    "    folder_name = f'average_{train_val_test}_r{reach}'\n",
    "    file_name = f'average_{year}_{train_val_test}_r{reach}.csv'\n",
    "    \n",
    "    full_path = os.path.join(dir_averages, folder_name, file_name)\n",
    "    \n",
    "    if os.path.exists(full_path):\n",
    "        # Read CSV without header/index\n",
    "        return pd.read_csv(full_path, header=None).values\n",
    "    else:\n",
    "        # Fallback if specific year is missing (return None or zeros)\n",
    "        # print(f\"Warning: Average for {year} not found at {full_path}\")\n",
    "        return None\n",
    "\n",
    "# --- CORRECTED FUNCTIONS ---\n",
    "\n",
    "def create_list_images(train_val_test, reach, dir_folders, collection):\n",
    "    '''\n",
    "    Robust version: Searches for the folder ending in '_r{reach}' \n",
    "    instead of guessing the full name.\n",
    "    '''\n",
    "    list_dir_images = []\n",
    "    \n",
    "    # 1. Find the correct reach folder inside the month folder (dir_folders)\n",
    "    if not os.path.exists(dir_folders):\n",
    "        print(f\"‚ùå Error: Directory not found: {dir_folders}\")\n",
    "        return []\n",
    "\n",
    "    target_folder_path = None\n",
    "    \n",
    "    # Search for folder ending in \"_r1\" (e.g.)\n",
    "    for folder_name in os.listdir(dir_folders):\n",
    "        # We check if it matches the reach ID\n",
    "        if folder_name.endswith(f'_r{reach}'):\n",
    "            target_folder_path = os.path.join(dir_folders, folder_name)\n",
    "            break\n",
    "            \n",
    "    if target_folder_path is None:\n",
    "        print(f\"‚ùå Error: No folder found for reach {reach} in {dir_folders}\")\n",
    "        return []\n",
    "\n",
    "    # 2. Collect .tif images\n",
    "    # Sort them to ensure years are in order (1988, 1989...)\n",
    "    sorted_files = sorted(os.listdir(target_folder_path))\n",
    "    \n",
    "    for image in sorted_files:\n",
    "        if image.endswith('.tif'):\n",
    "            path_image = os.path.join(target_folder_path, image)\n",
    "            list_dir_images.append(path_image)\n",
    "            \n",
    "    return list_dir_images\n",
    "\n",
    "def create_datasets(train_val_test, reach, year_target=5, nodata_value=-1, dir_folders=r'data\\satellite\\dataset', \n",
    "                    collection=r'JRC_GSW1_4_MonthlyHistory', scaled_classes=True):\n",
    "    \n",
    "    # 1. Get Images\n",
    "    list_dir_images = create_list_images(train_val_test, reach, dir_folders, collection)\n",
    "    \n",
    "    if not list_dir_images:\n",
    "        return [], [] # Return empty if path failed\n",
    "\n",
    "    # 2. Load Images into Arrays\n",
    "    images_array = []\n",
    "    valid_indices = [] # Keep track of which years actually loaded\n",
    "    \n",
    "    # We extract the year from the filename to match with averages\n",
    "    # Assuming filename format: ..._1988-03_r1.tif\n",
    "    loaded_years = []\n",
    "\n",
    "    for idx, path in enumerate(list_dir_images):\n",
    "        img = load_image_array(path, scaled_classes=scaled_classes)\n",
    "        if img is not None:\n",
    "            images_array.append(img)\n",
    "            \n",
    "            # Extract year safely\n",
    "            filename = os.path.basename(path)\n",
    "            # Find year (4 digits)\n",
    "            try:\n",
    "                # Split by '_' or '-' and find the item that looks like a year\n",
    "                parts = filename.replace('-', '_').split('_')\n",
    "                year = next(p for p in parts if p.isdigit() and len(p) == 4)\n",
    "                loaded_years.append(int(year))\n",
    "            except:\n",
    "                # Fallback if naming is weird, assume sequential start 1988\n",
    "                loaded_years.append(1988 + idx)\n",
    "\n",
    "    # 3. Load Averages\n",
    "    # Point to the correct averages directory\n",
    "    # If dir_folders is \".../Ganges_images/preprocessed/month_3\"\n",
    "    # We want \".../Ganges_images/averages\"\n",
    "    base_proj_dir = os.path.dirname(os.path.dirname(dir_folders)) # Go up two levels\n",
    "    dir_averages = os.path.join(base_proj_dir, 'averages')\n",
    "\n",
    "    avg_imgs = []\n",
    "    for year in loaded_years:\n",
    "        avg = load_avg(train_val_test, reach, year, dir_averages)\n",
    "        # If avg is missing for a year, use a zero-array as fallback to prevent crash\n",
    "        if avg is None:\n",
    "            avg = np.zeros_like(images_array[0]) \n",
    "        avg_imgs.append(avg)\n",
    "\n",
    "    # 4. Replace No-Data (Binary Conversion)\n",
    "    good_images_array = [np.where(image == nodata_value, avg_imgs[i], image) \n",
    "                         for i, image in enumerate(images_array)]\n",
    "        \n",
    "    input_dataset = []\n",
    "    target_dataset = []\n",
    "    \n",
    "    # 5. Create Sequences (n-to-1)\n",
    "    # Ensure we have enough images for the sequence\n",
    "    if len(good_images_array) < year_target:\n",
    "        print(f\"‚ö†Ô∏è Not enough images for Reach {reach} (Found {len(good_images_array)}, need {year_target})\")\n",
    "        return [], []\n",
    "\n",
    "    for i in range(len(good_images_array) - year_target + 1):\n",
    "        # Input: Sequence of (year_target - 1) images\n",
    "        input_seq = good_images_array[i : i + year_target - 1]\n",
    "        \n",
    "        # Target: The next image\n",
    "        target_seq = [good_images_array[i + year_target - 1]]\n",
    "        \n",
    "        input_dataset.append(input_seq)\n",
    "        target_dataset.append(target_seq)\n",
    "\n",
    "    return input_dataset, target_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from: data\\satellite\\Ganges_images\\preprocessed\\month_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathi\\miniconda3\\envs\\braided\\lib\\site-packages\\osgeo\\gdal.py:330: FutureWarning: Neither gdal.UseExceptions() nor gdal.DontUseExceptions() has been explicitly called. In GDAL 4.0, exceptions will be enabled by default.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Created 19 samples.\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION ---\n",
    "rivers = ['Jamuna','Ganges', 'Indus', 'Ghangara']\n",
    "river = rivers[1] # Ganges\n",
    "\n",
    "base_dir = os.path.join('data', 'satellite', f'{river}_images')\n",
    "dir_proc = os.path.join(base_dir, 'preprocessed')\n",
    "\n",
    "# Point explicitly to Month 3 (March)\n",
    "dir_dataset_mar = os.path.join(dir_proc, 'month_3')\n",
    "\n",
    "print(f\"Reading from: {dir_dataset_mar}\")\n",
    "\n",
    "# --- EXECUTION ---\n",
    "# Note: 'train_val_test' string is used mainly for finding the Average file\n",
    "input_mar, target_mar = create_datasets(\n",
    "    train_val_test='training', # Use string 'training', not variable 'train'\n",
    "    reach=1, \n",
    "    year_target=5, \n",
    "    dir_folders=dir_dataset_mar\n",
    ")\n",
    "\n",
    "print(f\"Success! Created {len(input_mar)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting dataset generation for training (Reach 1)...\n",
      "\n",
      "Processing Month 1...\n",
      "   > Generated 19 samples.\n",
      "\n",
      "Processing Month 2...\n",
      "   > Generated 19 samples.\n",
      "\n",
      "Processing Month 3...\n",
      "   > Generated 19 samples.\n",
      "\n",
      "Processing Month 4...\n",
      "   > Generated 19 samples.\n",
      "\n",
      "‚úÖ Done! Variables input_jan, input_feb, etc. are ready.\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION ---\n",
    "train_val_test = 'training'  # Ensure this is a string\n",
    "reach_id = 1                 # The reach you are targeting\n",
    "prediction_horizon = 5       # 5th year prediction\n",
    "# ---------------------\n",
    "\n",
    "# Dictionary to store results: {'month_1': (input, target), 'month_2': ...}\n",
    "all_datasets = {}\n",
    "\n",
    "print(f\"üöÄ Starting dataset generation for {train_val_test} (Reach {reach_id})...\\n\")\n",
    "\n",
    "for month in range(1, 5):\n",
    "    # 1. Construct the path dynamically (month_1, month_2, etc.)\n",
    "    month_folder = os.path.join(dir_proc, f'month_{month}')\n",
    "    \n",
    "    # 2. Check if folder exists\n",
    "    if not os.path.exists(month_folder):\n",
    "        print(f\"‚ö†Ô∏è  Skipping Month {month}: Folder not found at {month_folder}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing Month {month}...\")\n",
    "\n",
    "    # 3. Create Dataset for this month\n",
    "    # We pass the string 'training' explicitly\n",
    "    inputs, targets = create_datasets(\n",
    "        train_val_test=train_val_test, \n",
    "        reach=reach_id, \n",
    "        year_target=prediction_horizon, \n",
    "        dir_folders=month_folder\n",
    "    )\n",
    "\n",
    "    # 4. Store results\n",
    "    all_datasets[f'month_{month}'] = (inputs, targets)\n",
    "    \n",
    "    print(f\"   > Generated {len(inputs)} samples.\\n\")\n",
    "\n",
    "# --- OPTIONAL: Unpack into variables if you specifically need them ---\n",
    "# This matches your original variable naming scheme\n",
    "input_jan, target_jan = all_datasets.get('month_1', ([], []))\n",
    "input_feb, target_feb = all_datasets.get('month_2', ([], []))\n",
    "input_mar, target_mar = all_datasets.get('month_3', ([], []))\n",
    "input_apr, target_apr = all_datasets.get('month_4', ([], []))\n",
    "\n",
    "print(\"‚úÖ Done! Variables input_jan, input_feb, etc. are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_datasets(train_val_test, reach, year_target=5, \n",
    "                     nonwater_threshold=480000, nodata_value=-1, nonwater_value=0,   \n",
    "                     dir_folders=r'data\\satellite\\dataset', \n",
    "                     collection=r'JRC_GSW1_4_MonthlyHistory', scaled_classes=True):\n",
    "    '''\n",
    "    Filters image sequences. If any image in the input sequence (or the target) \n",
    "    has too many non-water pixels (implying it's just a placeholder or bad data),\n",
    "    the whole sequence is discarded.\n",
    "    '''\n",
    "    \n",
    "    # 1. Generate the raw dataset using your previous function\n",
    "    # Note: We rely on the create_datasets function we fixed earlier\n",
    "    input_dataset, target_dataset = create_datasets(\n",
    "        train_val_test, reach, year_target, nodata_value, \n",
    "        dir_folders, collection, scaled_classes\n",
    "    )\n",
    "\n",
    "    filtered_input_dataset = []\n",
    "    filtered_target_dataset = []\n",
    "\n",
    "    print(f\"  Filtering {len(input_dataset)} sequences (Threshold: < {nonwater_threshold} non-water pixels)...\")\n",
    "\n",
    "    # 2. Filter pairs\n",
    "    for input_images, target_image_seq in zip(input_dataset, target_dataset):\n",
    "        \n",
    "        # Check Inputs: Are ALL images in the sequence \"good\"?\n",
    "        # (Good = non-water count is BELOW the threshold, meaning there is enough water)\n",
    "        is_input_good = True\n",
    "        for img in input_images:\n",
    "            # Check non-water pixels (value 0)\n",
    "            n_nonwater = np.sum(img == nonwater_value)\n",
    "            if n_nonwater >= nonwater_threshold:\n",
    "                is_input_good = False\n",
    "                break\n",
    "        \n",
    "        if is_input_good:\n",
    "            # Check Target: Is the target image also \"good\"?\n",
    "            # target_image_seq is a list [img], so we take [0]\n",
    "            target_img = target_image_seq[0]\n",
    "            n_nonwater_target = np.sum(target_img == nonwater_value)\n",
    "            \n",
    "            if n_nonwater_target < nonwater_threshold:\n",
    "                # Both input sequence and target are valid\n",
    "                filtered_input_dataset.append(input_images)\n",
    "                filtered_target_dataset.append(target_img) # Store as raw image, not list\n",
    "\n",
    "    print(f\"  > Kept {len(filtered_input_dataset)} / {len(input_dataset)} sequences.\")\n",
    "    \n",
    "    return filtered_input_dataset, filtered_target_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Filtering for training (Reach 1)...\n",
      "\n",
      "Processing Month 1...\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 17 / 19 sequences.\n",
      "\n",
      "Processing Month 2...\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 18 / 19 sequences.\n",
      "\n",
      "Processing Month 3...\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 14 / 19 sequences.\n",
      "\n",
      "Processing Month 4...\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 18 / 19 sequences.\n",
      "\n",
      "‚úÖ Filtering Complete. Filtered variables are ready.\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION ---\n",
    "train_val_test = 'training'\n",
    "reach_id = 1\n",
    "prediction_horizon = 5\n",
    "\n",
    "# Dictionary to hold the final filtered data\n",
    "# Structure: {'month_1': (inputs, targets), ...}\n",
    "final_datasets = {}\n",
    "\n",
    "print(f\"üöÄ Starting Filtering for {train_val_test} (Reach {reach_id})...\\n\")\n",
    "\n",
    "for month in range(1, 5):\n",
    "    # Construct path: .../preprocessed/month_1\n",
    "    month_path = os.path.join(dir_proc, f'month_{month}')\n",
    "    \n",
    "    if not os.path.exists(month_path):\n",
    "        print(f\"Skipping Month {month} (Path not found)\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"Processing Month {month}...\")\n",
    "    \n",
    "    # Run Filter\n",
    "    inputs, targets = combine_datasets(\n",
    "        train_val_test=train_val_test, \n",
    "        reach=reach_id, \n",
    "        dir_folders=month_path,\n",
    "        year_target=prediction_horizon\n",
    "    )\n",
    "    \n",
    "    # Store in dictionary\n",
    "    final_datasets[f'month_{month}'] = (inputs, targets)\n",
    "    print(\"\") # Empty line for readability\n",
    "\n",
    "# --- UNPACKING (If you need individual variables) ---\n",
    "input_jan_filtered, target_jan_filtered = final_datasets.get('month_1', ([], []))\n",
    "input_feb_filtered, target_feb_filtered = final_datasets.get('month_2', ([], []))\n",
    "input_mar_filtered, target_mar_filtered = final_datasets.get('month_3', ([], []))\n",
    "input_apr_filtered, target_apr_filtered = final_datasets.get('month_4', ([], []))\n",
    "\n",
    "print(\"‚úÖ Filtering Complete. Filtered variables are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jan: 17 samples\n",
      "Feb: 18 samples\n",
      "Mar: 14 samples\n",
      "Apr: 18 samples\n"
     ]
    }
   ],
   "source": [
    "print(f\"Jan: {len(input_jan_filtered)} samples\")\n",
    "print(f\"Feb: {len(input_feb_filtered)} samples\")\n",
    "print(f\"Mar: {len(input_mar_filtered)} samples\")\n",
    "print(f\"Apr: {len(input_apr_filtered)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "def create_full_dataset(train_val_test, year_target=5, \n",
    "                        nonwater_threshold=480000, nodata_value=-1, nonwater_value=0, \n",
    "                        dir_folders=r'data\\satellite\\dataset', \n",
    "                        collection=r'JRC_GSW1_4_MonthlyHistory', \n",
    "                        scaled_classes=True, device='cuda:0', dtype=torch.float32):\n",
    "    '''\n",
    "    Combines ALL reaches within a specific month folder into a single TensorDataset.\n",
    "    Optimized to convert numpy arrays to tensors efficiently.\n",
    "    '''\n",
    "    \n",
    "    # initialize lists\n",
    "    all_inputs = []\n",
    "    all_targets = []\n",
    "    \n",
    "    # 1. Scan the folder for valid reach sub-directories\n",
    "    if not os.path.exists(dir_folders):\n",
    "        print(f\"‚ùå Error: Path not found: {dir_folders}\")\n",
    "        return TensorDataset(torch.empty(0), torch.empty(0))\n",
    "\n",
    "    # Look for folders like \"JRC_..._r1\", \"JRC_..._r8\", etc.\n",
    "    potential_folders = [f for f in os.listdir(dir_folders) if os.path.isdir(os.path.join(dir_folders, f))]\n",
    "    \n",
    "    print(f\"üìÇ Scanning {dir_folders}...\")\n",
    "\n",
    "    count_reaches = 0\n",
    "    \n",
    "    for folder_name in potential_folders:\n",
    "        # We need to extract the Reach ID from the folder name.\n",
    "        # Naming convention is expected to be: \"..._r{number}\"\n",
    "        try:\n",
    "            reach_id_str = folder_name.split('_r')[-1]\n",
    "            reach_id = int(reach_id_str)\n",
    "        except (IndexError, ValueError):\n",
    "            # Skip folders that don't match the \"_rX\" pattern\n",
    "            continue\n",
    "            \n",
    "        # 2. Get data for this specific reach\n",
    "        # combine_datasets performs the loading + filtering\n",
    "        inputs, targets = combine_datasets(\n",
    "            train_val_test, \n",
    "            reach=reach_id, \n",
    "            year_target=year_target, \n",
    "            nonwater_threshold=nonwater_threshold,\n",
    "            nodata_value=nodata_value, \n",
    "            nonwater_value=nonwater_value, \n",
    "            dir_folders=dir_folders, # Passes the month folder\n",
    "            collection=collection, \n",
    "            scaled_classes=scaled_classes\n",
    "        )\n",
    "        \n",
    "        if len(inputs) > 0:\n",
    "            all_inputs.extend(inputs)\n",
    "            all_targets.extend(targets)\n",
    "            count_reaches += 1\n",
    "            \n",
    "    print(f\"   > Aggregated data from {count_reaches} reaches.\")\n",
    "\n",
    "    if len(all_inputs) == 0:\n",
    "        print(\"‚ö†Ô∏è Warning: No valid samples found after filtering.\")\n",
    "        return TensorDataset(torch.empty(0), torch.empty(0))\n",
    "\n",
    "    # 3. Convert list of numpy arrays to a single Torch Tensor\n",
    "    # Converting numpy list -> numpy array -> torch tensor is much faster/safer than list -> tensor\n",
    "    \n",
    "    try:\n",
    "        # Convert list of lists/arrays into a single large numpy array\n",
    "        input_np = np.array(all_inputs) \n",
    "        target_np = np.array(all_targets)\n",
    "        \n",
    "        # Create Tensors on the specified device\n",
    "        input_tensor = torch.tensor(input_np, dtype=dtype, device=device)\n",
    "        target_tensor = torch.tensor(target_np, dtype=dtype, device=device)\n",
    "        \n",
    "        dataset = TensorDataset(input_tensor, target_tensor)\n",
    "        return dataset\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating tensors (Check memory or array shapes): {e}\")\n",
    "        return TensorDataset(torch.empty(0), torch.empty(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Creating Full Datasets on cuda:0...\n",
      "\n",
      "Processing Jan...\n",
      "üìÇ Scanning data\\satellite\\Ganges_images\\preprocessed\\month_1...\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 17 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 17 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 6 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 6 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 17 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 17 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 15 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 14 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 14 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 9 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 6 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 4 / 19 sequences.\n",
      "   > Aggregated data from 12 reaches.\n",
      "   > Created dataset with 142 samples.\n",
      "\n",
      "Processing Feb...\n",
      "üìÇ Scanning data\\satellite\\Ganges_images\\preprocessed\\month_2...\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 18 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 18 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 16 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 16 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 18 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 16 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 9 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 9 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 18 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 18 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 18 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 18 / 19 sequences.\n",
      "   > Aggregated data from 12 reaches.\n",
      "   > Created dataset with 192 samples.\n",
      "\n",
      "Processing Mar...\n",
      "üìÇ Scanning data\\satellite\\Ganges_images\\preprocessed\\month_3...\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 14 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 18 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 18 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 18 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 18 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 18 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 16 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 16 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 18 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 18 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 18 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 18 / 19 sequences.\n",
      "   > Aggregated data from 12 reaches.\n",
      "   > Created dataset with 208 samples.\n",
      "\n",
      "Processing Apr...\n",
      "üìÇ Scanning data\\satellite\\Ganges_images\\preprocessed\\month_4...\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 18 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 16 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 16 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 16 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 18 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 18 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 18 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 18 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 13 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 13 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 11 / 19 sequences.\n",
      "  Filtering 19 sequences (Threshold: < 480000 non-water pixels)...\n",
      "  > Kept 11 / 19 sequences.\n",
      "   > Aggregated data from 12 reaches.\n",
      "   > Created dataset with 186 samples.\n",
      "\n",
      "Total training samples considering different months:\n",
      "January --> 142\n",
      "February --> 192\n",
      "March --> 208\n",
      "April --> 186\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION ---\n",
    "train_val_test = 'training'  # Make sure this is the string 'training'\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "target_dtype = torch.float32\n",
    "\n",
    "# Store datasets in a dictionary or list\n",
    "datasets_by_month = {}\n",
    "\n",
    "print(f\"üöÄ Creating Full Datasets on {device}...\\n\")\n",
    "\n",
    "# Loop Jan - Apr\n",
    "for month_idx, month_dir in zip(\n",
    "    ['Jan', 'Feb', 'Mar', 'Apr'], \n",
    "    [dir_dataset_jan, dir_dataset_feb, dir_dataset_mar, dir_dataset_apr]\n",
    "):\n",
    "    print(f\"Processing {month_idx}...\")\n",
    "    \n",
    "    ds = create_full_dataset(\n",
    "        train_val_test=train_val_test, \n",
    "        dir_folders=month_dir, \n",
    "        device=device, \n",
    "        dtype=target_dtype\n",
    "    )\n",
    "    \n",
    "    datasets_by_month[month_idx] = ds\n",
    "    print(f\"   > Created dataset with {len(ds)} samples.\\n\")\n",
    "\n",
    "\n",
    "# Print Summary\n",
    "print(f'Total training samples considering different months:\\n\\\n",
    "January --> {len(datasets_by_month[\"Jan\"])}\\n\\\n",
    "February --> {len(datasets_by_month[\"Feb\"])}\\n\\\n",
    "March --> {len(datasets_by_month[\"Mar\"])}\\n\\\n",
    "April --> {len(datasets_by_month[\"Apr\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue normal code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Input and target shape month by month (training reach 1):\\n\\\n",
    "# March --> input shape: {np.shape(input_mar)} - Target shape: {np.shape(target_mar)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_jan, target_jan = create_datasets(train, 1, 5, dir_folders=dir_dataset_jan)\n",
    "# input_feb, target_feb = create_datasets(train, 1, 5, dir_folders=dir_dataset_feb)\n",
    "# input_mar, target_mar = create_datasets(train, 1, 5, dir_folders=dir_dataset_mar)\n",
    "# input_apr, target_apr = create_datasets(train, 1, 5, dir_folders=dir_dataset_apr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input and target shape month by month (training reach 1):\n",
      "January --> input shape: (19, 4, 1000, 500) - Target shape: (19, 1, 1000, 500)\n",
      "February --> input shape: (19, 4, 1000, 500) - Target shape: (19, 1, 1000, 500)\n",
      "March --> input shape: (19, 4, 1000, 500) - Target shape: (19, 1, 1000, 500)\n",
      "April --> input shape: (19, 4, 1000, 500) - Target shape: (19, 1, 1000, 500)\n"
     ]
    }
   ],
   "source": [
    "print(f'Input and target shape month by month (training reach 1):\\n\\\n",
    "January --> input shape: {np.shape(input_jan)} - Target shape: {np.shape(target_jan)}\\n\\\n",
    "February --> input shape: {np.shape(input_feb)} - Target shape: {np.shape(target_feb)}\\n\\\n",
    "March --> input shape: {np.shape(input_mar)} - Target shape: {np.shape(target_mar)}\\n\\\n",
    "April --> input shape: {np.shape(input_apr)} - Target shape: {np.shape(target_apr)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Combine input and target datasets filtering out bad images (based on <code>no-data</code> and <code>water</code> thresholds). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_jan_filtered, target_jan_filtered = combine_datasets(train, 1, dir_folders=dir_dataset_jan)\n",
    "input_feb_filtered, target_feb_filtered = combine_datasets(train, 1, dir_folders=dir_dataset_feb)\n",
    "input_mar_filtered, target_mar_filtered = combine_datasets(train, 1, dir_folders=dir_dataset_mar)\n",
    "input_apr_filtered, target_apr_filtered = combine_datasets(train, 1, dir_folders=dir_dataset_apr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input and target shape month by month after filtering out not suitable images (training reach 1):\n",
      "January --> input shape: (6, 4, 1000, 500) - Target shape: (6, 1000, 500)\n",
      "February --> input shape: (17, 4, 1000, 500) - Target shape: (17, 1000, 500)\n",
      "March --> input shape: (13, 4, 1000, 500) - Target shape: (13, 1000, 500)\n",
      "April --> input shape: (10, 4, 1000, 500) - Target shape: (10, 1000, 500)\n"
     ]
    }
   ],
   "source": [
    "print(f'Input and target shape month by month after filtering out not suitable images (training reach 1):\\n\\\n",
    "January --> input shape: {np.shape(input_jan_filtered)} - Target shape: {np.shape(target_jan_filtered)}\\n\\\n",
    "February --> input shape: {np.shape(input_feb_filtered)} - Target shape: {np.shape(target_feb_filtered)}\\n\\\n",
    "March --> input shape: {np.shape(input_mar_filtered)} - Target shape: {np.shape(target_mar_filtered)}\\n\\\n",
    "April --> input shape: {np.shape(input_apr_filtered)} - Target shape: {np.shape(target_apr_filtered)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples considering different months:\n",
      "January --> 378\n",
      "February --> 402\n",
      "March --> 413\n",
      "April --> 262\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "dtype = dtype=torch.float32\n",
    "\n",
    "dataset_train_jan = create_full_dataset(train, dir_folders=dir_dataset_jan, device=device, dtype=dtype)\n",
    "dataset_train_feb = create_full_dataset(train, dir_folders=dir_dataset_feb, device=device, dtype=dtype)\n",
    "dataset_train_mar = create_full_dataset(train, dir_folders=dir_dataset_mar, device=device, dtype=dtype)\n",
    "dataset_train_apr = create_full_dataset(train, dir_folders=dir_dataset_apr, device=device, dtype=dtype)\n",
    "\n",
    "print(f'Total training samples considering different months:\\n\\\n",
    "January --> {len(dataset_train_jan)}\\n\\\n",
    "February --> {len(dataset_train_feb)}\\n\\\n",
    "March --> {len(dataset_train_mar)}\\n\\\n",
    "April --> {len(dataset_train_apr)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets shape (same for every monthly dataset)\n",
      "Input dataset sample shape: torch.Size([4, 1000, 500]) - Target dataset sample shape: torch.Size([1000, 500])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Datasets shape (same for every monthly dataset)\\n\\\n",
    "Input dataset sample shape: {dataset_train_jan[0][0].shape} - Target dataset sample shape: {dataset_train_jan[0][1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total validation samples considering different months:\n",
      "January --> 9\n",
      "February --> 19\n",
      "March --> 13\n",
      "April --> 17\n"
     ]
    }
   ],
   "source": [
    "# validation\n",
    "dataset_val_jan = create_full_dataset(val, dir_folders=dir_dataset_jan, device=device, dtype=dtype)\n",
    "dataset_val_feb = create_full_dataset(val, dir_folders=dir_dataset_feb, device=device, dtype=dtype)\n",
    "dataset_val_mar = create_full_dataset(val, dir_folders=dir_dataset_mar, device=device, dtype=dtype)\n",
    "dataset_val_apr = create_full_dataset(val, dir_folders=dir_dataset_apr, device=device, dtype=dtype)\n",
    "\n",
    "print(f'Total validation samples considering different months:\\n\\\n",
    "January --> {len(dataset_val_jan)}\\n\\\n",
    "February --> {len(dataset_val_feb)}\\n\\\n",
    "March --> {len(dataset_val_mar)}\\n\\\n",
    "April --> {len(dataset_val_apr)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total validation samples considering different months:\n",
      "January --> 16\n",
      "February --> 19\n",
      "March --> 17\n",
      "April --> 17\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "dataset_test_jan = create_full_dataset(test, dir_folders=dir_dataset_jan, device=device, dtype=dtype)\n",
    "dataset_test_feb = create_full_dataset(test, dir_folders=dir_dataset_feb, device=device, dtype=dtype)\n",
    "dataset_test_mar = create_full_dataset(test, dir_folders=dir_dataset_mar, device=device, dtype=dtype)\n",
    "dataset_test_apr = create_full_dataset(test, dir_folders=dir_dataset_apr, device=device, dtype=dtype)\n",
    "\n",
    "print(f'Total validation samples considering different months:\\n\\\n",
    "January --> {len(dataset_test_jan)}\\n\\\n",
    "February --> {len(dataset_test_feb)}\\n\\\n",
    "March --> {len(dataset_test_mar)}\\n\\\n",
    "April --> {len(dataset_test_apr)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "braided",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
